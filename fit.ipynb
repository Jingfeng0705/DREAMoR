{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec7b0e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, os\n",
    "\n",
    "import importlib, time\n",
    "import traceback\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dreamor.utils.config_new import ConfigParser\n",
    "from dreamor.utils.logging import Logger, class_name_to_file_name, mkdir, cp_files\n",
    "from dreamor.utils.torch import load_state\n",
    "from dreamor.fitting.fitting_utils import NSTAGES, load_vposer, save_optim_result\n",
    "from dreamor.fitting.motion_optimizer import MotionOptimizer\n",
    "\n",
    "from dreamor.body_model.body_model import BodyModel\n",
    "\n",
    "from dreamor.body_model.utils import SMPLH_PATH\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "160908ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default: {'imapper_scene_subseq_idx', 'floor_reg_weight', 'mask_joints2d', 'amass_use_joints', 'amass_use_points', 'humor_steps_in', 'imapper_seq_len', 'humor_model_data_config', 'robust_tuning_const', 'humor', 'amass_root_joint_only', 'vposer', 'imapper_scene', 'rgb_intrinsics', 'humor_out_rot_rep', 'prox_recording', 'prox_batch_size', 'batch_size', 'stage3_contact_refine_only', 'lbfgs_max_iter', 'stage3_tune_init_state', 'rgb_overlap_consist_weight', 'robust_loss', 'save_stages_results', 'joint2d_weight', 'rgb_seq_len', 'rgb_planercnn_res', 'amass_custom_split', 'op_keypts', 'joint3d_rollout_weight', 'rgb_overlap_len', 'joint2d_sigma', 'humor_latent_size', 'humor_in_rot_rep', 'openpose', 'prox_recording_subseq_idx', 'prox_seq_len', 'amass_drop_middle'}\n",
      "Using default: {'output_delta', 'detach_sched_samp', 'model_use_smpl_joint_inputs'}\n"
     ]
    }
   ],
   "source": [
    "config_file = r\"configs\\fit_keypts_humor_diffusion_transformer.yaml\"\n",
    "config_parser_yaml = ConfigParser(config_file)\n",
    "args_obj, _ = config_parser_yaml.parse('fit')\n",
    "args = args_obj.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f53ace0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_dict: {'data_path': '../datasets/AMASS/amass_processed', 'data_type': 'AMASS', 'data_fps': 30, 'batch_size': 1, 'shuffle': False, 'op_keypts': None, 'amass_split_by': 'sequence', 'amass_custom_split': None, 'amass_batch_size': 1, 'amass_seq_len': 60, 'amass_use_joints': False, 'amass_root_joint_only': False, 'amass_use_verts': True, 'amass_use_points': False, 'amass_noise_std': 0.0, 'amass_make_partial': True, 'amass_partial_height': 0.9, 'amass_drop_middle': False, 'prox_batch_size': -1, 'prox_seq_len': 60, 'prox_recording': None, 'prox_recording_subseq_idx': -1, 'imapper_seq_len': 60, 'imapper_scene': None, 'imapper_scene_subseq_idx': -1, 'rgb_seq_len': None, 'rgb_overlap_len': None, 'rgb_intrinsics': None, 'rgb_planercnn_res': None, 'rgb_overlap_consist_weight': [0.0, 0.0, 0.0], 'mask_joints2d': False, 'joint3d_weight': [0.0, 0.0, 0.0], 'joint3d_rollout_weight': [0.0, 0.0, 0.0], 'joint3d_smooth_weight': [0.1, 0.1, 0.0], 'vert3d_weight': [1.0, 1.0, 1.0], 'point3d_weight': [0.0, 0.0, 0.0], 'joint2d_weight': [0.0, 0.0, 0.0], 'pose_prior_weight': [0.0002, 0.0002, 0.0], 'shape_prior_weight': [0.000167, 0.000167, 0.000167], 'motion_prior_weight': [0.0, 0.0, 0.0005], 'init_motion_prior_weight': [0.0, 0.0, 0.0], 'joint_consistency_weight': [0.0, 0.0, 1.0], 'bone_length_weight': [0.0, 0.0, 10.0], 'contact_vel_weight': [0.0, 0.0, 1.0], 'contact_height_weight': [0.0, 0.0, 1.0], 'floor_reg_weight': [0.0, 0.0, 0.0], 'robust_loss': 'bisquare', 'robust_tuning_const': 4.6851, 'joint2d_sigma': 100.0, 'stage3_tune_init_state': True, 'stage3_tune_init_num_frames': 15, 'stage3_tune_init_freeze_start': 30, 'stage3_tune_init_freeze_end': 55, 'stage3_contact_refine_only': True, 'smpl': './body_models/smplh/neutral/model.npz', 'gt_body_type': 'smplh', 'vposer': './body_models/vposer_v1_0', 'openpose': './external/openpose', 'humor': None, 'humor_out_rot_rep': 'aa', 'humor_in_rot_rep': 'mat', 'humor_latent_size': 48, 'humor_model_data_config': 'smpl+joints+contacts', 'humor_steps_in': 1, 'init_motion_prior': './checkpoints/init_state_prior_gmm', 'lr': 1.0, 'num_iters': [30, 70, 70], 'lbfgs_max_iter': 20, 'out': './out/eval/128_full_stage3_ddimstep10_bestmodel', 'save_results': True, 'save_stages_results': False, 'model': 'DreamorDiffusionTransformer', 'ckpt': './out/humordiffusiontransformer_train/20250507_040645/checkpoints/best_model.pth'}\n",
      "model_dict: {'out_rot_rep': 'aa', 'in_rot_rep': 'mat', 'latent_size': 128, 'steps_in': 1, 'output_delta': True, 'model_data_config': 'smpl+joints+contacts', 'detach_sched_samp': True, 'model_use_smpl_joint_inputs': False, 'pose_token_dim': 64, 'diffusion_base_dim': 256, 'nhead': 4, 'num_layers': 6, 'dim_feedforward': 1024, 'dropout': 0.1, 'cfg_scale': 4.0, 'cond_drop_prob': 0.1, 'use_mean_sample': True, 'ddim_steps': 10}\n",
      "dataset_dict: None\n",
      "loss_dict: None\n"
     ]
    }
   ],
   "source": [
    "# See config\n",
    "dict_attr = ['base_dict', 'model_dict', 'dataset_dict', 'loss_dict']\n",
    "for attr in dict_attr:\n",
    "    print(f\"{attr}: {getattr(args_obj, attr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "144fdd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: ./out/eval/128_full_stage3_ddimstep10_bestmodel\n"
     ]
    }
   ],
   "source": [
    "res_out_path = None\n",
    "if args.out is not None:\n",
    "    print(f\"Output directory: {args.out}\")\n",
    "    mkdir(args.out)\n",
    "    # create logging system\n",
    "    fit_log_path = os.path.join(args.out, 'fit_' + str(int(time.time())) + '.log')\n",
    "    Logger.init(fit_log_path)\n",
    "\n",
    "    if args.save_results or args.save_stages_results:\n",
    "        res_out_path = os.path.join(args.out, 'results_out')\n",
    "cp_files(args.out, [config_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a219f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "B = args.batch_size\n",
    "if args.amass_batch_size > 0:\n",
    "    B = args.amass_batch_size\n",
    "if args.prox_batch_size > 0:\n",
    "    B = args.prox_batch_size\n",
    "if B == 3:\n",
    "    Logger.log('Cannot use batch size 3, setting to 2!') # NOTE: bug with pytorch 3x3 matmul weirdness\n",
    "    B = 2\n",
    "data_fps = args.data_fps\n",
    "im_dim = (1080, 1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "beccef4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from../datasets/AMASS/amass_processed\n",
      "This split contains 425 sequences (that meet the duration criteria).\n",
      "The dataset contains 4198 sub-sequences in total.\n"
     ]
    }
   ],
   "source": [
    "from dreamor.datasets.amass_fit_dataset import AMASSFitDataset\n",
    "dataset = AMASSFitDataset(args.data_path,\n",
    "                            seq_len=args.amass_seq_len,\n",
    "                            return_joints=args.amass_use_joints,\n",
    "                            return_verts=args.amass_use_verts,\n",
    "                            return_points=args.amass_use_points,\n",
    "                            noise_std=args.amass_noise_std,\n",
    "                            make_partial=args.amass_make_partial,\n",
    "                            partial_height=args.amass_partial_height,\n",
    "                            drop_middle=args.amass_drop_middle,\n",
    "                            root_only=args.amass_root_joint_only,\n",
    "                            split_by=args.amass_split_by,\n",
    "                            custom_split=args.amass_custom_split)\n",
    "data_loader = DataLoader(dataset, \n",
    "                            batch_size=B,\n",
    "                            shuffle=args.shuffle,\n",
    "                            num_workers=0,\n",
    "                            pin_memory=True,\n",
    "                            drop_last=False,\n",
    "                            worker_init_fn=lambda _: np.random.seed())\n",
    "data_fps = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "228043e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights = {\n",
    "        'joints2d' : args.joint2d_weight,\n",
    "        'joints3d' : args.joint3d_weight,\n",
    "        'joints3d_rollout' : args.joint3d_rollout_weight,\n",
    "        'verts3d' : args.vert3d_weight,\n",
    "        'points3d' : args.point3d_weight,\n",
    "        'pose_prior' : args.pose_prior_weight,\n",
    "        'shape_prior' : args.shape_prior_weight,\n",
    "        'motion_prior' : args.motion_prior_weight,\n",
    "        'init_motion_prior' : args.init_motion_prior_weight,\n",
    "        'joint_consistency' : args.joint_consistency_weight,\n",
    "        'bone_length' : args.bone_length_weight,\n",
    "        'joints3d_smooth' : args.joint3d_smooth_weight,\n",
    "        'contact_vel' : args.contact_vel_weight,\n",
    "        'contact_height' : args.contact_height_weight,\n",
    "        'floor_reg' : args.floor_reg_weight,\n",
    "        'rgb_overlap_consist' : args.rgb_overlap_consist_weight\n",
    "    }\n",
    "\n",
    "max_loss_weights = {k : max(v) for k, v in loss_weights.items()}\n",
    "all_stage_loss_weights = []\n",
    "for sidx in range(NSTAGES):\n",
    "    stage_loss_weights = {k : v[sidx] for k, v in loss_weights.items()}\n",
    "    all_stage_loss_weights.append(stage_loss_weights)\n",
    "    \n",
    "use_joints2d = max_loss_weights['joints2d'] > 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f57d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Trained Model: ./body_models/vposer_v1_0\\snapshots\\TR00_E096.pt\n",
      "Loading motion prior from ./out/humordiffusiontransformer_train/20250507_040645/checkpoints/best_model.pth...\n",
      "Model class:  <module 'models.dreamor_diffusion_transformer' from 'E:\\\\workspace\\\\Motion\\\\humor\\\\dreamor\\\\utils\\\\..\\\\models\\\\dreamor_diffusion_transformer.py'>\n",
      "Using default: {'beta2', 'beta1', 'eps', 'decay', 'use_adam', 'load_optim', 'ckpt'}\n",
      "Using default: {'detach_sched_samp', 'output_delta', 'model_use_smpl_joint_inputs'}\n",
      "Using default: {'data_noise_std', 'frames_out_step_size', 'splits_path'}\n",
      "Using default: {'smpl_vert_consistency_loss', 'kl_loss_cycle_len'}\n"
     ]
    }
   ],
   "source": [
    "from dreamor.models.dreamor_diffusion_transformer import DreamorDiffusionTransformer\n",
    "\n",
    "pose_prior, _ = load_vposer(args.vposer)\n",
    "pose_prior = pose_prior.to(device)\n",
    "pose_prior.eval()\n",
    "\n",
    "model_file = class_name_to_file_name(args.model)\n",
    "Logger.log('Loading motion prior from %s...' % (args.ckpt))\n",
    "vae_ckpt_path = r'checkpoints\\motionvae\\best_model.pth'\n",
    "vae_cfg_path = r'checkpoints\\motionvae\\train_motion_vae.yaml'\n",
    "\n",
    "model_class = importlib.import_module('models.' + model_file)\n",
    "print('Model class: ', model_class)\n",
    "Model = getattr(model_class, args.model)\n",
    "\n",
    "if args.model == 'DreamorDiffusionTransformer':\n",
    "    motion_prior = Model(**args_obj.model_dict,\n",
    "                    vae_cfg=vae_cfg_path, vae_ckpt=vae_ckpt_path,\n",
    "                    model_smpl_batch_size=args.batch_size)\n",
    "else:\n",
    "    motion_prior = Model(**args_obj.model_dict,\n",
    "                  model_smpl_batch_size=args.batch_size)\n",
    "\n",
    "motion_prior.to(device)\n",
    "load_state(args.ckpt, motion_prior, map_location=device)\n",
    "motion_prior.eval()\n",
    "\n",
    "init_motion_prior = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef454750",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_batch_overlap_res_dict = None\n",
    "use_overlap_loss = sum(loss_weights['rgb_overlap_consist']) > 0.0\n",
    "\n",
    "skip_chunk = 1720\n",
    "for i, data in enumerate(data_loader):\n",
    "    if (i % skip_chunk) != 0:\n",
    "        continue\n",
    "    start_t = time.time()\n",
    "    # these dicts have different data depending on modality\n",
    "    # MUST have at least name\n",
    "    observed_data, gt_data = data\n",
    "    \n",
    "    name = gt_data['name'][0]\n",
    "    Logger.log('Processing sequence %s' % (name))\n",
    "    # both of these are a list of tuples, each list index is a frame and the tuple index is the seq within the batch\n",
    "    obs_img_paths = None if 'img_paths' not in observed_data else observed_data['img_paths'] \n",
    "    obs_mask_paths = None if 'mask_paths' not in observed_data else observed_data['mask_paths']\n",
    "    observed_data = {k : v.to(device) for k, v in observed_data.items() if isinstance(v, torch.Tensor)}\n",
    "    for k, v in gt_data.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            gt_data[k] = v.to(device)\n",
    "    cur_batch_size = observed_data[list(observed_data.keys())[0]].size(0)\n",
    "    T = observed_data[list(observed_data.keys())[0]].size(1)\n",
    "\n",
    "    if use_overlap_loss and 'seq_interval' not in observed_data:\n",
    "        print('Must have frame index labels from data to determine overlap')\n",
    "        exit()\n",
    "\n",
    "    if cur_batch_size == 3:\n",
    "        # NOTE: hacky way to avoid bug with pytorch 3x3 matmul problems with batch size 3....\n",
    "        for k, v in observed_data.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                observed_data[k] = torch.cat([v, v[-1:]], dim=0)\n",
    "            else:\n",
    "                # otherwise it's a list\n",
    "                observed_data[k] = v + [v[-1]]\n",
    "        for k, v in gt_data.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                gt_data[k] = torch.cat([v, v[-1:]], dim=0)\n",
    "            else:\n",
    "                # otherwise it's a list\n",
    "                gt_data[k] = v + [v[-1]]\n",
    "                if k == 'name':\n",
    "                    # change name so we don't overwrite\n",
    "                    gt_data[k][-1] = gt_data[k][-1] + '_copy'\n",
    "\n",
    "        # obs_img_paths and obs_mask_paths\n",
    "        if obs_img_paths is not None:\n",
    "            new_obs_img_paths = []\n",
    "            for img_paths in obs_img_paths:\n",
    "                new_obs_img_paths.append(img_paths + [img_paths[-1]])\n",
    "            obs_img_paths = new_obs_img_paths\n",
    "        if obs_mask_paths is not None:\n",
    "            new_obs_mask_paths = []\n",
    "            for mask_paths in obs_mask_paths:\n",
    "                new_obs_mask_paths.append(mask_paths + [mask_paths[-1]])\n",
    "            obs_mask_paths = new_obs_mask_paths\n",
    "\n",
    "        cur_batch_size = 4\n",
    "\n",
    "    # pass in the last batch index from previous batch is using overlap consistency\n",
    "    if use_overlap_loss and prev_batch_overlap_res_dict is not None:\n",
    "        observed_data['prev_batch_overlap_res'] = prev_batch_overlap_res_dict\n",
    "\n",
    "    seq_names = []\n",
    "    for gt_idx, gt_name in enumerate(gt_data['name']):\n",
    "        seq_name = gt_name + '_' + str(int(time.time())) + str(gt_idx)\n",
    "        Logger.log(seq_name)\n",
    "        seq_names.append(seq_name)\n",
    "\n",
    "    cur_z_init_paths = []\n",
    "    cur_z_final_paths = []\n",
    "    cur_res_out_paths = []\n",
    "    for seq_name in seq_names:\n",
    "        # set current out paths based on sequence name\n",
    "        if res_out_path is not None:\n",
    "            cur_res_out_path = os.path.join(res_out_path, seq_name)\n",
    "            mkdir(cur_res_out_path)\n",
    "            cur_res_out_paths.append(cur_res_out_path)\n",
    "    cur_res_out_paths = cur_res_out_paths if len(cur_res_out_paths) > 0 else None\n",
    "    if cur_res_out_paths is not None and args.data_type == 'RGB' and args.save_results:\n",
    "        all_res_out_paths += cur_res_out_paths\n",
    "    cur_z_init_paths = cur_z_init_paths if len(cur_z_init_paths) > 0 else None\n",
    "    cur_z_final_paths = cur_z_final_paths if len(cur_z_final_paths) > 0 else None\n",
    "\n",
    "    # get body model\n",
    "    # load in from given path\n",
    "    Logger.log('Loading SMPL model from %s...' % (args.smpl))\n",
    "    body_model_path = args.smpl\n",
    "    fit_gender = body_model_path.split('/')[-2]\n",
    "    num_betas = 16 if 'betas' not in gt_data else gt_data['betas'].size(2)\n",
    "    body_model = BodyModel(bm_path=body_model_path,\n",
    "                            num_betas=num_betas,\n",
    "                            batch_size=cur_batch_size*T,\n",
    "                            use_vtx_selector=use_joints2d).to(device)\n",
    "\n",
    "    if body_model.model_type != 'smplh':\n",
    "        print('Only SMPL+H model is supported')\n",
    "        exit()\n",
    "\n",
    "    gt_body_paths = None\n",
    "    if 'gender' in gt_data:\n",
    "        gt_body_paths = []\n",
    "        for cur_gender in gt_data['gender']:\n",
    "            gt_body_path = None\n",
    "            if args.gt_body_type == 'smplh':\n",
    "                gt_body_path = os.path.join(SMPLH_PATH, '%s/model.npz' % (cur_gender))\n",
    "            gt_body_paths.append(gt_body_path)\n",
    "\n",
    "    cam_mat = None\n",
    "    if 'cam_matx' in gt_data:\n",
    "        cam_mat = gt_data['cam_matx'].to(device)\n",
    "\n",
    "    #  save meta results information about the optimized bm and GT bm (gender)\n",
    "    if args.save_results:\n",
    "        for bidx, cur_res_out_path in enumerate(cur_res_out_paths):\n",
    "            cur_meta_path = os.path.join(cur_res_out_path, 'meta.txt')\n",
    "            with open(cur_meta_path, 'w') as f:\n",
    "                f.write('optim_bm %s\\n' % (body_model_path))\n",
    "                if gt_body_paths is None:\n",
    "                    f.write('gt_bm %s\\n' % (body_model_path))\n",
    "                else:\n",
    "                    f.write('gt_bm %s\\n' % (gt_body_paths[bidx]))\n",
    "\n",
    "    # create optimizer\n",
    "    optimizer = MotionOptimizer(device,\n",
    "                                body_model,\n",
    "                                num_betas,\n",
    "                                cur_batch_size,\n",
    "                                T,\n",
    "                                [k for k in observed_data.keys()],\n",
    "                                all_stage_loss_weights,\n",
    "                                pose_prior,\n",
    "                                motion_prior,\n",
    "                                init_motion_prior,\n",
    "                                use_joints2d,\n",
    "                                cam_mat,\n",
    "                                args.robust_loss,\n",
    "                                args.robust_tuning_const,\n",
    "                                args.joint2d_sigma,\n",
    "                                stage3_tune_init_state=args.stage3_tune_init_state,\n",
    "                                stage3_tune_init_num_frames=args.stage3_tune_init_num_frames,\n",
    "                                stage3_tune_init_freeze_start=args.stage3_tune_init_freeze_start,\n",
    "                                stage3_tune_init_freeze_end=args.stage3_tune_init_freeze_end,\n",
    "                                stage3_contact_refine_only=args.stage3_contact_refine_only,\n",
    "                                use_chamfer=('points3d' in observed_data),\n",
    "                                im_dim=im_dim)\n",
    "\n",
    "    # run optimizer\n",
    "    try:\n",
    "        optim_result, per_stage_results = optimizer.run(observed_data,\n",
    "                                                        data_fps=data_fps,\n",
    "                                                        lr=args.lr,\n",
    "                                                        num_iter=args.num_iters,\n",
    "                                                        lbfgs_max_iter=args.lbfgs_max_iter,\n",
    "                                                        stages_res_out=cur_res_out_paths,\n",
    "                                                        fit_gender=fit_gender,\n",
    "                                                        fit_log_path=fit_log_path,)\n",
    "\n",
    "        # save final results\n",
    "        if cur_res_out_paths is not None:\n",
    "            save_optim_result(cur_res_out_paths, optim_result, per_stage_results, gt_data, observed_data, args.data_type,\n",
    "                                optim_floor=optimizer.optim_floor,\n",
    "                                obs_img_paths=obs_img_paths,\n",
    "                                obs_mask_paths=obs_mask_paths)\n",
    "\n",
    "        elapsed_t = time.time() - start_t\n",
    "        Logger.log('Optimized sequence %d in %f s' % (i, elapsed_t))\n",
    "\n",
    "        # cache last verts, floor, and betas from last batch index to use in consistency loss\n",
    "        #   for next batch\n",
    "        if use_overlap_loss:\n",
    "            prev_batch_overlap_res_dict = dict()\n",
    "            prev_batch_overlap_res_dict['verts3d'] = per_stage_results['stage3']['verts3d'][-1].clone().detach()\n",
    "            prev_batch_overlap_res_dict['betas'] = optim_result['betas'][-1].clone().detach()\n",
    "            prev_batch_overlap_res_dict['floor_plane'] = optim_result['floor_plane'][-1].clone().detach()\n",
    "            prev_batch_overlap_res_dict['seq_interval'] = observed_data['seq_interval'][-1].clone().detach()\n",
    "\n",
    "    except Exception as e:\n",
    "        Logger.log('Caught error in current optimization! Skipping...')\n",
    "        Logger.log(traceback.format_exc())\n",
    "\n",
    "    if i < (len(data_loader) - 1):\n",
    "        del optimizer\n",
    "    del body_model\n",
    "    del observed_data\n",
    "    del gt_data\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS280",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
