{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b219eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, os\n",
    "\n",
    "import importlib, time\n",
    "import traceback\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from humor.utils.config_new import ConfigParser\n",
    "from humor.utils.logging import Logger, class_name_to_file_name, mkdir, cp_files\n",
    "from humor.utils.torch import get_device, save_state, load_state\n",
    "from humor.utils.stats import StatTracker\n",
    "from humor.models.model_utils import step\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3e566a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default: {'eps', 'ckpt', 'use_adam', 'load_optim', 'beta1', 'beta2', 'decay'}\n",
      "Using default: {'model_use_smpl_joint_inputs', 'detach_sched_samp', 'output_delta'}\n",
      "Using default: {'frames_out_step_size', 'splits_path', 'data_noise_std'}\n",
      "Using default: {'kl_loss_cycle_len', 'smpl_vert_consistency_loss'}\n",
      "base_dict: {'dataset': 'AmassDiscreteDataset', 'model': 'MotionVAE', 'loss': 'HumorLoss', 'out': './out/motion_vae', 'ckpt': None, 'gpu': 0, 'batch_size': 200, 'print_every': 10, 'epochs': 200, 'val_every': 2, 'save_every': 25, 'lr': 0.001, 'beta1': 0.9, 'beta2': 0.999, 'eps': 1e-08, 'sched_milestones': [50, 80, 140], 'sched_decay': 0.5, 'decay': 0.0, 'load_optim': True, 'use_adam': False, 'sched_samp_start': 150, 'sched_samp_end': 160}\n",
      "model_dict: {'out_rot_rep': 'aa', 'in_rot_rep': 'mat', 'latent_size': 128, 'steps_in': 1, 'output_delta': True, 'model_data_config': 'smpl+joints+contacts', 'detach_sched_samp': True, 'model_use_smpl_joint_inputs': False}\n",
      "dataset_dict: {'data_paths': ['../datasets/AMASS/amass_processed'], 'split_by': 'sequence', 'splits_path': None, 'sample_num_frames': 60, 'data_rot_rep': 'mat', 'step_frames_in': 1, 'step_frames_out': 1, 'frames_out_step_size': 1, 'data_return_config': 'smpl+joints+contacts', 'data_noise_std': 0.0}\n",
      "loss_dict: {'kl_loss': 0.0004, 'kl_loss_anneal_start': 0, 'kl_loss_anneal_end': 50, 'kl_loss_cycle_len': -1, 'regr_trans_loss': 1.0, 'regr_trans_vel_loss': 1.0, 'regr_root_orient_loss': 1.0, 'regr_root_orient_vel_loss': 1.0, 'regr_pose_loss': 1.0, 'regr_pose_vel_loss': 1.0, 'regr_joint_loss': 1.0, 'regr_joint_vel_loss': 1.0, 'regr_joint_orient_vel_loss': 1.0, 'regr_vert_loss': 1.0, 'regr_vert_vel_loss': 1.0, 'contacts_loss': 0.01, 'contacts_vel_loss': 0.01, 'smpl_joint_loss': 1.0, 'smpl_mesh_loss': 1.0, 'smpl_joint_consistency_loss': 1.0, 'smpl_vert_consistency_loss': 0.0}\n"
     ]
    }
   ],
   "source": [
    "config_path = r\"configs\\train_motion_vae.yaml\"\n",
    "config_parser_yaml = ConfigParser(config_path)\n",
    "args_obj, _ = config_parser_yaml.parse('train')\n",
    "# See config\n",
    "dict_attr = ['base_dict', 'model_dict', 'dataset_dict', 'loss_dict']\n",
    "for attr in dict_attr:\n",
    "    print(f\"{attr}: {getattr(args_obj, attr)}\")\n",
    "args = args_obj.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e457022c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: {'out_rot_rep': 'aa', 'in_rot_rep': 'mat', 'latent_size': 128, 'steps_in': 1, 'output_delta': True, 'model_data_config': 'smpl+joints+contacts', 'detach_sched_samp': True, 'model_use_smpl_joint_inputs': False, 'pose_token_dim': 256, 'diffusion_base_dim': 256, 'nhead': 4, 'num_layers': 6, 'dim_feedforward': 1024, 'dropout': 0.1, 'cfg_scale': 4.0}\n",
      "Using default: {'ckpt', 'decay', 'eps', 'use_adam', 'load_optim', 'beta1', 'beta2'}\n",
      "Using default: {'detach_sched_samp', 'output_delta', 'model_use_smpl_joint_inputs'}\n",
      "Using default: {'data_noise_std', 'frames_out_step_size', 'splits_path'}\n",
      "Using default: {'smpl_vert_consistency_loss', 'kl_loss_cycle_len'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\workspace\\Motion\\humor\\humor\\utils\\torch.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  full_checkpoint_dict = torch.load(load_path, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "from humor.models.humor_diffusion_transformer import HumorDiffusionTransformer\n",
    "print(f\"Model: {args_obj.model_dict}\")\n",
    "\n",
    "model = HumorDiffusionTransformer(**args_obj.model_dict,\n",
    "                                  vae_ckpt=r'out\\motion_vae\\20250506_014121\\checkpoints\\best_model.pth',\n",
    "                                  vae_cfg=r'configs\\train_motion_vae.yaml',\n",
    "                                  model_smpl_batch_size=args.batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b98427c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from humor.models.motion_v_a_e import MotionVAE\n",
    "vae = MotionVAE(**args_obj.model_dict,\n",
    "                 model_smpl_batch_size=args.batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f752850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using detected GPU...\n"
     ]
    }
   ],
   "source": [
    "device = get_device(args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e69ec6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\workspace\\Motion\\humor\\humor\\utils\\torch.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  full_checkpoint_dict = torch.load(load_path, map_location=map_location)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(131, 0.009386874148801256, 0.008887221386369066)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.to(device)\n",
    "vae_ckpt = r'out\\motion_vae\\20250506_014121\\checkpoints\\best_train_model.pth'\n",
    "load_state(vae_ckpt, vae, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b98e682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # load loss class and instantiate\n",
    "from humor.losses.humor_loss import HumorLoss\n",
    "loss_func = HumorLoss(**args_obj.loss_dict,\n",
    "                    smpl_batch_size=args.batch_size*args_obj.dataset.sample_num_frames) # assumes loss is HumorLoss\n",
    "loss_func.to(device)\n",
    "# freeze params in loss\n",
    "for param in loss_func.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1527e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from../datasets/AMASS/amass_processed\n",
      "Logger must be initialized before logging!\n",
      "This split contains 423 sequences (that meet the duration criteria).\n",
      "Logger must be initialized before logging!\n",
      "The dataset contains 4136 sub-sequences in total.\n",
      "Logger must be initialized before logging!\n"
     ]
    }
   ],
   "source": [
    "from humor.datasets. amass_discrete_dataset import AmassDiscreteDataset\n",
    "\n",
    "test_dataset = AmassDiscreteDataset(split='test', **args_obj.dataset_dict,)\n",
    "# create loaders\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                            batch_size=4,\n",
    "                            shuffle=False, \n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c7faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_chunk = 172\n",
    "ctn = 0\n",
    "for i, data in enumerate(test_loader):\n",
    "    if (i % skip_chunk) != 0:\n",
    "        continue\n",
    "    print(f\"i: {i}\")\n",
    "    print(f\"ctn: {ctn}\")\n",
    "    ctn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a59351af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57191269",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "vae.to(device)\n",
    "vae.eval()\n",
    "for i, data in enumerate(test_loader):\n",
    "    try:\n",
    "        # run model\n",
    "        loss, stats_dict = step(vae, loss_func, data, test_dataset, device, cur_epoch=0, mode='train', use_gt_p=1.0)\n",
    "        if torch.isnan(loss).item():\n",
    "            Logger.log('WARNING: NaN loss. Skipping to next data...')\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "    except (RuntimeError, AssertionError) as e:\n",
    "        Logger.log(f'WARNING: {e}')\n",
    "        raise e\n",
    "\n",
    "    losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "703209b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pred_noise', 'z', 'z_noise'])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "x_input = torch.randn(2000, 1, 339).to(device)\n",
    "x_prev = torch.randn(2000, 1, 339).to(device)\n",
    "\n",
    "out = model(x_prev, x_input)\n",
    "print(out.keys())  # (2000, 1, 339)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed65f6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.013733987061035199\n"
     ]
    }
   ],
   "source": [
    "print(f\"loss: {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29606a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_list: [3, 3, 9, 3, 189, 66, 66]\n",
      "339\n"
     ]
    }
   ],
   "source": [
    "input_list = model.input_dim_list\n",
    "print(f\"output_list: {input_list}\")\n",
    "print(sum(input_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a11272d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiffusionTransformer(\n",
       "  (pose_tokenizer): PoseTokenizer(\n",
       "    (part_proj): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=3, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=9, out_features=256, bias=True)\n",
       "      (3): Linear(in_features=3, out_features=256, bias=True)\n",
       "      (4): Linear(in_features=189, out_features=256, bias=True)\n",
       "      (5-6): 2 x Linear(in_features=66, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (latent_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (pose_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  )\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from humor.models.diffusion_transformer import DiffusionTransformer\n",
    "\n",
    "diff_model = DiffusionTransformer(128, 256, input_list)\n",
    "diff_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6731b189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: torch.Size([2000, 128])\n"
     ]
    }
   ],
   "source": [
    "x_input = torch.randn(2000, 339).to(device)\n",
    "z_input = torch.randn(2000, 128).to(device)\n",
    "t=torch.randint(0, 1000, (2000,)).to(device)\n",
    "output = diff_model(z_input, x_input, t)\n",
    "print(f\"output: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c6d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from humor.fitting.config import parse_args\n",
    "\n",
    "original_cfg_path = r'configs\\fit_amass_keypts.cfg'\n",
    "yaml_cfg_path = r'configs\\fit_amass_keypts.yaml'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS280",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
