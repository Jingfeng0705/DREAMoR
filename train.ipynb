{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec7b0e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, os\n",
    "\n",
    "import importlib, time\n",
    "import traceback\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from humor.utils.config_new import ConfigParser\n",
    "from humor.utils.logging import Logger, class_name_to_file_name, mkdir, cp_files\n",
    "from humor.utils.torch import get_device, save_state, load_state\n",
    "from humor.utils.stats import StatTracker\n",
    "from humor.models.model_utils import step\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "160908ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default: {'beta1', 'ckpt', 'beta2', 'decay', 'load_optim', 'eps', 'use_adam'}\n",
      "Using default: {'detach_sched_samp', 'model_use_smpl_joint_inputs', 'output_delta'}\n",
      "Using default: {'splits_path', 'frames_out_step_size', 'data_noise_std'}\n",
      "Using default: {'kl_loss_cycle_len', 'smpl_vert_consistency_loss'}\n"
     ]
    }
   ],
   "source": [
    "config_file = \"configs/train_testmodel.yaml\"\n",
    "config_parser_yaml = ConfigParser(config_file)\n",
    "args_obj, _ = config_parser_yaml.parse('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f53ace0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_dict: {'dataset': 'AmassDiscreteDataset', 'model': 'TestModel', 'loss': 'HumorLoss', 'out': './out/testmodel_train_test', 'ckpt': None, 'gpu': 0, 'batch_size': 200, 'print_every': 10, 'epochs': 200, 'val_every': 2, 'save_every': 25, 'lr': 0.0001, 'beta1': 0.9, 'beta2': 0.999, 'eps': 1e-08, 'sched_milestones': [50, 80, 140], 'sched_decay': 0.5, 'decay': 0.0, 'load_optim': True, 'use_adam': False, 'sched_samp_start': 2, 'sched_samp_end': 10}\n",
      "model_dict: {'out_rot_rep': 'aa', 'in_rot_rep': 'mat', 'latent_size': 48, 'steps_in': 1, 'conditional_prior': False, 'output_delta': True, 'model_data_config': 'smpl+joints+contacts', 'detach_sched_samp': True, 'model_use_smpl_joint_inputs': False}\n",
      "dataset_dict: {'data_paths': ['../datasets/AMASS/amass_processed'], 'split_by': 'sequence', 'splits_path': None, 'sample_num_frames': 10, 'data_rot_rep': 'mat', 'step_frames_in': 1, 'step_frames_out': 1, 'frames_out_step_size': 1, 'data_return_config': 'smpl+joints+contacts', 'data_noise_std': 0.0}\n",
      "loss_dict: {'kl_loss': 0.0004, 'kl_loss_anneal_start': 0, 'kl_loss_anneal_end': 50, 'kl_loss_cycle_len': -1, 'regr_trans_loss': 1.0, 'regr_trans_vel_loss': 1.0, 'regr_root_orient_loss': 1.0, 'regr_root_orient_vel_loss': 1.0, 'regr_pose_loss': 1.0, 'regr_pose_vel_loss': 1.0, 'regr_joint_loss': 1.0, 'regr_joint_vel_loss': 1.0, 'regr_joint_orient_vel_loss': 1.0, 'regr_vert_loss': 1.0, 'regr_vert_vel_loss': 1.0, 'contacts_loss': 0.01, 'contacts_vel_loss': 0.01, 'smpl_joint_loss': 1.0, 'smpl_mesh_loss': 1.0, 'smpl_joint_consistency_loss': 1.0, 'smpl_vert_consistency_loss': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# See config\n",
    "dict_attr = ['base_dict', 'model_dict', 'dataset_dict', 'loss_dict']\n",
    "for attr in dict_attr:\n",
    "    print(f\"{attr}: {getattr(args_obj, attr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cba45594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base args: namespace(dataset='AmassDiscreteDataset', model='TestModel', loss='HumorLoss', out='./out/testmodel_train_test\\\\20250424_145116', ckpt=None, gpu=0, batch_size=200, print_every=10, epochs=200, val_every=2, save_every=25, lr=0.0001, beta1=0.9, beta2=0.999, eps=1e-08, sched_milestones=[50, 80, 140], sched_decay=0.5, decay=0.0, load_optim=True, use_adam=False, sched_samp_start=2, sched_samp_end=10)\n",
      "Model args: namespace(out_rot_rep='aa', in_rot_rep='mat', latent_size=48, steps_in=1, conditional_prior=False, output_delta=True, model_data_config='smpl+joints+contacts', detach_sched_samp=True, model_use_smpl_joint_inputs=False)\n",
      "Dataset args: namespace(data_paths=['../datasets/AMASS/amass_processed'], split_by='sequence', splits_path=None, sample_num_frames=10, data_rot_rep='mat', step_frames_in=1, step_frames_out=1, frames_out_step_size=1, data_return_config='smpl+joints+contacts', data_noise_std=0.0)\n",
      "Loss args: namespace(kl_loss=0.0004, kl_loss_anneal_start=0, kl_loss_anneal_end=50, kl_loss_cycle_len=-1, regr_trans_loss=1.0, regr_trans_vel_loss=1.0, regr_root_orient_loss=1.0, regr_root_orient_vel_loss=1.0, regr_pose_loss=1.0, regr_pose_vel_loss=1.0, regr_joint_loss=1.0, regr_joint_vel_loss=1.0, regr_joint_orient_vel_loss=1.0, regr_vert_loss=1.0, regr_vert_vel_loss=1.0, contacts_loss=0.01, contacts_vel_loss=0.01, smpl_joint_loss=1.0, smpl_mesh_loss=1.0, smpl_joint_consistency_loss=1.0, smpl_vert_consistency_loss=0.0)\n"
     ]
    }
   ],
   "source": [
    "args = args_obj.base\n",
    "args.out = os.path.join(args.out, time.strftime('%Y%m%d_%H%M%S'))\n",
    "mkdir(args.out)\n",
    "train_log_path = os.path.join(args.out, 'train.log')\n",
    "Logger.init(train_log_path)\n",
    "   \n",
    "# save arguments used\n",
    "Logger.log('Base args: ' + str(args))\n",
    "Logger.log('Model args: ' + str(args_obj.model))\n",
    "Logger.log('Dataset args: ' + str(args_obj.dataset))\n",
    "Logger.log('Loss args: ' + str(args_obj.loss))\n",
    "\n",
    "train_scripts_path = os.path.join(args.out, 'train_scripts')\n",
    "mkdir(train_scripts_path)\n",
    "pkg_root = \"humor\"\n",
    "dataset_file = class_name_to_file_name(args.dataset)\n",
    "dataset_file_path = os.path.join(pkg_root, 'datasets/' + dataset_file + '.py')\n",
    "model_file = class_name_to_file_name(args.model)\n",
    "model_file_path = os.path.join(pkg_root, 'models/' + model_file + '.py')\n",
    "loss_file = class_name_to_file_name(args.loss)\n",
    "train_file_path = \"train.ipynb\"\n",
    "cp_files(train_scripts_path, [train_file_path, model_file_path, dataset_file_path, config_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13e55124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model class:  <module 'models.test_model' from 'E:\\\\workspace\\\\Motion\\\\humor_local\\\\humor\\\\utils\\\\..\\\\models\\\\test_model.py'>\n"
     ]
    }
   ],
   "source": [
    "model_class = importlib.import_module('models.' + model_file)\n",
    "print('Model class: ', model_class)\n",
    "Model = getattr(model_class, args.model)\n",
    "model = Model(**args_obj.model_dict,\n",
    "                model_smpl_batch_size=args.batch_size) # assumes model is HumorModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1313e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss class:  <module 'losses.humor_loss' from 'E:\\\\workspace\\\\Motion\\\\humor_local\\\\humor\\\\utils\\\\..\\\\losses\\\\humor_loss.py'>\n"
     ]
    }
   ],
   "source": [
    "  # load loss class and instantiate\n",
    "loss_class = importlib.import_module('losses.' + loss_file)\n",
    "print('Loss class: ', loss_class)\n",
    "Loss = getattr(loss_class, args.loss)\n",
    "loss_func = Loss(**args_obj.loss_dict,\n",
    "                    smpl_batch_size=args.batch_size*args_obj.dataset.sample_num_frames) # assumes loss is HumorLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "641c2f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using detected GPU...\n",
      "TestModel(\n",
      "  (mlp_test): Sequential(\n",
      "    (0): Linear(in_features=339, out_features=512, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (4): ELU(alpha=1.0)\n",
      "    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (7): ELU(alpha=1.0)\n",
      "    (8): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): Linear(in_features=512, out_features=96, bias=True)\n",
      "  )\n",
      "  (decode_test): Sequential(\n",
      "    (0): Linear(in_features=48, out_features=512, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (4): ELU(alpha=1.0)\n",
      "    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (7): ELU(alpha=1.0)\n",
      "    (8): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): Linear(in_features=512, out_features=216, bias=True)\n",
      "  )\n",
      ")\n",
      "Num model params: 2467640\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = get_device(args.gpu)\n",
    "model.to(device)\n",
    "loss_func.to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# count params in model\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "Logger.log('Num model params: ' + str(params))\n",
    "\n",
    "# freeze params in loss\n",
    "for param in loss_func.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93b1fca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using scheduled sampling starting at epoch 2 and ending at epoch 10!\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "betas = (args.beta1, args.beta2)\n",
    "if args.use_adam:\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                            lr=args.lr,\n",
    "                            betas=betas,\n",
    "                            eps=args.eps,\n",
    "                            weight_decay=args.decay)\n",
    "else:\n",
    "    optimizer = optim.Adamax(model.parameters(),\n",
    "                            lr=args.lr,\n",
    "                            betas=betas,\n",
    "                            eps=args.eps,\n",
    "                            weight_decay=args.decay)\n",
    "\n",
    "# load in pretrained weights/optimizer state if given\n",
    "start_epoch = 0\n",
    "min_val_loss = min_train_loss = float('inf')\n",
    "if args.ckpt is not None:\n",
    "    load_optim = optimizer if args.load_optim else None\n",
    "    start_epoch, min_val_loss, min_train_loss = load_state(args.ckpt, model, optimizer=load_optim, map_location=device, ignore_keys=model.ignore_keys)\n",
    "    start_epoch += 1\n",
    "    Logger.log('Resuming from saved checkpoint at epoch idx %d with min val loss %.6f...' % (start_epoch, min_val_loss))\n",
    "    if not args.load_optim:\n",
    "        Logger.log('Not loading optimizer state as desired...')\n",
    "        Logger.log('WARNING: Also resetting min_val_loss and epoch count!')\n",
    "        min_val_loss = float('inf')\n",
    "        start_epoch = 0\n",
    "\n",
    "# initialize LR scheduler\n",
    "scheduler = MultiStepLR(optimizer, milestones=args.sched_milestones, gamma=args.sched_decay)\n",
    "\n",
    "# intialize schedule sampling if desired\n",
    "use_sched_samp = False\n",
    "if args.sched_samp_start is not None and args.sched_samp_end is not None:\n",
    "    if args.sched_samp_start >= 0 and args.sched_samp_end >= args.sched_samp_start:\n",
    "        Logger.log('Using scheduled sampling starting at epoch %d and ending at epoch %d!' % (args.sched_samp_start, args.sched_samp_end))\n",
    "        use_sched_samp = True\n",
    "    else:\n",
    "        Logger.log('Could not use scheduled sampling with given start and end!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "035ae111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class:  <class 'datasets.amass_discrete_dataset.AmassDiscreteDataset'>\n",
      "Loading data from../datasets/AMASS/amass_processed\n",
      "Logger must be initialized before logging!\n",
      "This split contains 286 sequences (that meet the duration criteria).\n",
      "Logger must be initialized before logging!\n",
      "The dataset contains 18411 sub-sequences in total.\n",
      "Logger must be initialized before logging!\n",
      "Loading data from../datasets/AMASS/amass_processed\n",
      "Logger must be initialized before logging!\n",
      "This split contains 24 sequences (that meet the duration criteria).\n",
      "Logger must be initialized before logging!\n",
      "The dataset contains 739 sub-sequences in total.\n",
      "Logger must be initialized before logging!\n"
     ]
    }
   ],
   "source": [
    "# load dataset class and instantiate training and validation set\n",
    "Dataset = getattr(importlib.import_module('datasets.' + dataset_file), args.dataset)\n",
    "print('Dataset class: ', Dataset)\n",
    "train_dataset = Dataset(split='train', **args_obj.dataset_dict)\n",
    "val_dataset = Dataset(split='val', **args_obj.dataset_dict)\n",
    "# create loaders\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            pin_memory=True,\n",
    "                            worker_init_fn=lambda _: np.random.seed()) # get around pytorch RNG seed bug\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=args.batch_size,\n",
    "                        shuffle=False, \n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=True,\n",
    "                        worker_init_fn=lambda _: np.random.seed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93bd7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats tracker\n",
    "tensorboard_path = os.path.join(args.out, 'train_tensorboard')\n",
    "mkdir(tensorboard_path)\n",
    "stat_tracker = StatTracker(tensorboard_path, train_log_path)\n",
    "\n",
    "# checkpoints saving\n",
    "ckpts_path = os.path.join(args.out, 'checkpoints')\n",
    "mkdir(ckpts_path)\n",
    "\n",
    "if use_sched_samp:\n",
    "    train_dataset.return_global = True\n",
    "    val_dataset.return_global = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ce692d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled sampling current use_gt_p = 1.000000\n",
      "[>--------------------------------------------------] train epoch - 1/200 | batch - 1/93\n",
      "1.088 s per batch | 2 s elapsed | 23 d 23 h 2 m 4 s ETA\n",
      "train/kl_loss : 13.10291\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.32664\n",
      "train/trans_vel_loss : 0.39371\n",
      "train/root_orient_loss : 0.16943\n",
      "train/root_orient_vel_loss : 0.90141\n",
      "train/pose_body_loss : 0.18343\n",
      "train/joints_loss : 0.32518\n",
      "train/joints_vel_loss : 0.44647\n",
      "train/contacts_loss : 0.73437\n",
      "train/contacts_acc : 0.49772\n",
      "train/contacts_pos_acc : 0.49615\n",
      "train/contacts_neg_acc : 0.49838\n",
      "train/contacts_vel_loss : 2.22637\n",
      "train/smpl_joint_loss : 0.39690\n",
      "train/smpl_mesh_loss : 0.43292\n",
      "train/smpl_joint_consistency_loss : 0.72709\n",
      "train/reconstr_weighted_loss : 4.33280\n",
      "train/loss : 4.33280\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 1.08832\n",
      "[=====>---------------------------------------------] train epoch - 1/200 | batch - 11/93\n",
      "0.497 s per batch | 12 s elapsed | 6 h 35 m 42 s ETA\n",
      "train/kl_loss : 16.80646\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.07741\n",
      "train/trans_vel_loss : 0.14716\n",
      "train/root_orient_loss : 0.07864\n",
      "train/root_orient_vel_loss : 0.55361\n",
      "train/pose_body_loss : 0.16355\n",
      "train/joints_loss : 0.25420\n",
      "train/joints_vel_loss : 0.38986\n",
      "train/contacts_loss : 0.73216\n",
      "train/contacts_acc : 0.50408\n",
      "train/contacts_pos_acc : 0.49843\n",
      "train/contacts_neg_acc : 0.50640\n",
      "train/contacts_vel_loss : 1.92410\n",
      "train/smpl_joint_loss : 0.12380\n",
      "train/smpl_mesh_loss : 0.14809\n",
      "train/smpl_joint_consistency_loss : 0.37095\n",
      "train/reconstr_weighted_loss : 2.33382\n",
      "train/loss : 2.33382\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.49678\n",
      "[===========>---------------------------------------] train epoch - 1/200 | batch - 21/93\n",
      "0.469 s per batch | 22 s elapsed | 5 h 53 m 27 s ETA\n",
      "train/kl_loss : 19.88063\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.05399\n",
      "train/trans_vel_loss : 0.10176\n",
      "train/root_orient_loss : 0.05421\n",
      "train/root_orient_vel_loss : 0.50799\n",
      "train/pose_body_loss : 0.14248\n",
      "train/joints_loss : 0.20104\n",
      "train/joints_vel_loss : 0.34465\n",
      "train/contacts_loss : 0.72959\n",
      "train/contacts_acc : 0.50609\n",
      "train/contacts_pos_acc : 0.50565\n",
      "train/contacts_neg_acc : 0.50630\n",
      "train/contacts_vel_loss : 1.84104\n",
      "train/smpl_joint_loss : 0.08914\n",
      "train/smpl_mesh_loss : 0.10854\n",
      "train/smpl_joint_consistency_loss : 0.28242\n",
      "train/reconstr_weighted_loss : 1.91194\n",
      "train/loss : 1.91194\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.46854\n",
      "[================>----------------------------------] train epoch - 1/200 | batch - 31/93\n",
      "0.455 s per batch | 32 s elapsed | 5 h 38 m 42 s ETA\n",
      "train/kl_loss : 22.32804\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.04314\n",
      "train/trans_vel_loss : 0.08266\n",
      "train/root_orient_loss : 0.04361\n",
      "train/root_orient_vel_loss : 0.48148\n",
      "train/pose_body_loss : 0.12488\n",
      "train/joints_loss : 0.16544\n",
      "train/joints_vel_loss : 0.30946\n",
      "train/contacts_loss : 0.72649\n",
      "train/contacts_acc : 0.51028\n",
      "train/contacts_pos_acc : 0.51749\n",
      "train/contacts_neg_acc : 0.50732\n",
      "train/contacts_vel_loss : 1.71794\n",
      "train/smpl_joint_loss : 0.07202\n",
      "train/smpl_mesh_loss : 0.08822\n",
      "train/smpl_joint_consistency_loss : 0.23027\n",
      "train/reconstr_weighted_loss : 1.66561\n",
      "train/loss : 1.66561\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.45470\n",
      "[======================>----------------------------] train epoch - 1/200 | batch - 41/93\n",
      "0.450 s per batch | 42 s elapsed | 5 h 31 m 39 s ETA\n",
      "train/kl_loss : 24.47659\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.03663\n",
      "train/trans_vel_loss : 0.07125\n",
      "train/root_orient_loss : 0.03708\n",
      "train/root_orient_vel_loss : 0.46164\n",
      "train/pose_body_loss : 0.11075\n",
      "train/joints_loss : 0.14092\n",
      "train/joints_vel_loss : 0.28008\n",
      "train/contacts_loss : 0.72386\n",
      "train/contacts_acc : 0.51422\n",
      "train/contacts_pos_acc : 0.53092\n",
      "train/contacts_neg_acc : 0.50726\n",
      "train/contacts_vel_loss : 1.62257\n",
      "train/smpl_joint_loss : 0.06135\n",
      "train/smpl_mesh_loss : 0.07541\n",
      "train/smpl_joint_consistency_loss : 0.19562\n",
      "train/reconstr_weighted_loss : 1.49419\n",
      "train/loss : 1.49419\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.44952\n",
      "[===========================>-----------------------] train epoch - 1/200 | batch - 51/93\n",
      "0.447 s per batch | 53 s elapsed | 5 h 27 m 37 s ETA\n",
      "train/kl_loss : 26.54815\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.03217\n",
      "train/trans_vel_loss : 0.06393\n",
      "train/root_orient_loss : 0.03254\n",
      "train/root_orient_vel_loss : 0.45566\n",
      "train/pose_body_loss : 0.09938\n",
      "train/joints_loss : 0.12318\n",
      "train/joints_vel_loss : 0.25642\n",
      "train/contacts_loss : 0.72142\n",
      "train/contacts_acc : 0.51849\n",
      "train/contacts_pos_acc : 0.54664\n",
      "train/contacts_neg_acc : 0.50690\n",
      "train/contacts_vel_loss : 1.54371\n",
      "train/smpl_joint_loss : 0.05393\n",
      "train/smpl_mesh_loss : 0.06646\n",
      "train/smpl_joint_consistency_loss : 0.17100\n",
      "train/reconstr_weighted_loss : 1.37733\n",
      "train/loss : 1.37733\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.44692\n",
      "[================================>------------------] train epoch - 1/200 | batch - 61/93\n",
      "0.445 s per batch | 1 m 3 s elapsed | 5 h 25 m 40 s ETA\n",
      "train/kl_loss : 28.29996\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.02885\n",
      "train/trans_vel_loss : 0.05893\n",
      "train/root_orient_loss : 0.02924\n",
      "train/root_orient_vel_loss : 0.45434\n",
      "train/pose_body_loss : 0.09027\n",
      "train/joints_loss : 0.10996\n",
      "train/joints_vel_loss : 0.23720\n",
      "train/contacts_loss : 0.71930\n",
      "train/contacts_acc : 0.52244\n",
      "train/contacts_pos_acc : 0.56184\n",
      "train/contacts_neg_acc : 0.50615\n",
      "train/contacts_vel_loss : 1.45848\n",
      "train/smpl_joint_loss : 0.04841\n",
      "train/smpl_mesh_loss : 0.05976\n",
      "train/smpl_joint_consistency_loss : 0.15274\n",
      "train/reconstr_weighted_loss : 1.29149\n",
      "train/loss : 1.29149\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.44537\n",
      "[======================================>------------] train epoch - 1/200 | batch - 71/93\n",
      "0.451 s per batch | 1 m 19 s elapsed | 5 h 50 m 22 s ETA\n",
      "train/kl_loss : 29.99239\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.02625\n",
      "train/trans_vel_loss : 0.05498\n",
      "train/root_orient_loss : 0.02667\n",
      "train/root_orient_vel_loss : 0.44570\n",
      "train/pose_body_loss : 0.08280\n",
      "train/joints_loss : 0.09964\n",
      "train/joints_vel_loss : 0.22230\n",
      "train/contacts_loss : 0.71738\n",
      "train/contacts_acc : 0.52583\n",
      "train/contacts_pos_acc : 0.57734\n",
      "train/contacts_neg_acc : 0.50460\n",
      "train/contacts_vel_loss : 1.39412\n",
      "train/smpl_joint_loss : 0.04411\n",
      "train/smpl_mesh_loss : 0.05452\n",
      "train/smpl_joint_consistency_loss : 0.13851\n",
      "train/reconstr_weighted_loss : 1.21660\n",
      "train/loss : 1.21660\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.45071\n",
      "[===========================================>-------] train epoch - 1/200 | batch - 81/93\n",
      "0.455 s per batch | 1 m 39 s elapsed | 6 h 24 m 18 s ETA\n",
      "train/kl_loss : 31.54636\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.02416\n",
      "train/trans_vel_loss : 0.05166\n",
      "train/root_orient_loss : 0.02460\n",
      "train/root_orient_vel_loss : 0.44160\n",
      "train/pose_body_loss : 0.07660\n",
      "train/joints_loss : 0.09134\n",
      "train/joints_vel_loss : 0.21047\n",
      "train/contacts_loss : 0.71549\n",
      "train/contacts_acc : 0.52861\n",
      "train/contacts_pos_acc : 0.59281\n",
      "train/contacts_neg_acc : 0.50202\n",
      "train/contacts_vel_loss : 1.33661\n",
      "train/smpl_joint_loss : 0.04063\n",
      "train/smpl_mesh_loss : 0.05026\n",
      "train/smpl_joint_consistency_loss : 0.12708\n",
      "train/reconstr_weighted_loss : 1.15893\n",
      "train/loss : 1.15893\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.45525\n",
      "[================================================>--] train epoch - 1/200 | batch - 91/93\n",
      "0.459 s per batch | 1 m 59 s elapsed | 6 h 49 m 0 s ETA\n",
      "train/kl_loss : 32.97082\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.02242\n",
      "train/trans_vel_loss : 0.04891\n",
      "train/root_orient_loss : 0.02292\n",
      "train/root_orient_vel_loss : 0.43628\n",
      "train/pose_body_loss : 0.07138\n",
      "train/joints_loss : 0.08454\n",
      "train/joints_vel_loss : 0.20049\n",
      "train/contacts_loss : 0.71434\n",
      "train/contacts_acc : 0.53061\n",
      "train/contacts_pos_acc : 0.60592\n",
      "train/contacts_neg_acc : 0.49947\n",
      "train/contacts_vel_loss : 1.28979\n",
      "train/smpl_joint_loss : 0.03775\n",
      "train/smpl_mesh_loss : 0.04673\n",
      "train/smpl_joint_consistency_loss : 0.11769\n",
      "train/reconstr_weighted_loss : 1.10916\n",
      "train/loss : 1.10916\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.45851\n",
      "Saving checkpoint...\n",
      "Best train loss so far! Saving checkpoint...\n",
      "[============>--------------------------------------] val epoch - 1/200 | batch - 1/4\n",
      "0.343 s per batch | 2 m 4 s elapsed | 1436 d 16 h 24 m 18 s ETA\n",
      "val/kl_loss : 48.49272\n",
      "val/kl_anneal_weight : 0.00000\n",
      "val/kl_weighted_loss : 0.00000\n",
      "val/trans_loss : 0.00833\n",
      "val/trans_vel_loss : 0.02585\n",
      "val/root_orient_loss : 0.00911\n",
      "val/root_orient_vel_loss : 0.28589\n",
      "val/pose_body_loss : 0.02879\n",
      "val/joints_loss : 0.03034\n",
      "val/joints_vel_loss : 0.07970\n",
      "val/contacts_loss : 0.77009\n",
      "val/contacts_acc : 0.51261\n",
      "val/contacts_pos_acc : 0.78250\n",
      "val/contacts_neg_acc : 0.40160\n",
      "val/contacts_vel_loss : 0.98941\n",
      "val/smpl_joint_loss : 0.01457\n",
      "val/smpl_mesh_loss : 0.01847\n",
      "val/smpl_joint_consistency_loss : 0.04247\n",
      "val/reconstr_weighted_loss : 0.56111\n",
      "val/loss : 0.56111\n",
      "val/time_per_batch : 0.34330\n",
      "Best val loss so far! Saving checkpoint...\n",
      "Scheduled sampling current use_gt_p = 1.000000\n",
      "[>--------------------------------------------------] train epoch - 2/200 | batch - 1/93\n",
      "0.655 s per batch | 2 m 12 s elapsed | 7 h 19 m 48 s ETA\n",
      "train/kl_loss : 46.11703\n",
      "train/kl_anneal_weight : 0.02000\n",
      "train/kl_weighted_loss : 0.00037\n",
      "train/trans_loss : 0.00813\n",
      "train/trans_vel_loss : 0.02631\n",
      "train/root_orient_loss : 0.00885\n",
      "train/root_orient_vel_loss : 0.33385\n",
      "train/pose_body_loss : 0.02668\n",
      "train/joints_loss : 0.02729\n",
      "train/joints_vel_loss : 0.08742\n",
      "train/contacts_loss : 0.70335\n",
      "train/contacts_acc : 0.54517\n",
      "train/contacts_pos_acc : 0.72378\n",
      "train/contacts_neg_acc : 0.47192\n",
      "train/contacts_vel_loss : 0.84008\n",
      "train/smpl_joint_loss : 0.01379\n",
      "train/smpl_mesh_loss : 0.01720\n",
      "train/smpl_joint_consistency_loss : 0.03895\n",
      "train/reconstr_weighted_loss : 0.60390\n",
      "train/loss : 0.60427\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.65456\n",
      "[=====>---------------------------------------------] train epoch - 2/200 | batch - 11/93\n",
      "0.549 s per batch | 2 m 33 s elapsed | 7 h 40 m 3 s ETA\n",
      "train/kl_loss : 46.80986\n",
      "train/kl_anneal_weight : 0.02000\n",
      "train/kl_weighted_loss : 0.00037\n",
      "train/trans_loss : 0.00760\n",
      "train/trans_vel_loss : 0.02657\n",
      "train/root_orient_loss : 0.00852\n",
      "train/root_orient_vel_loss : 0.41579\n",
      "train/pose_body_loss : 0.02586\n",
      "train/joints_loss : 0.02657\n",
      "train/joints_vel_loss : 0.12771\n",
      "train/contacts_loss : 0.70179\n",
      "train/contacts_acc : 0.55062\n",
      "train/contacts_pos_acc : 0.72704\n",
      "train/contacts_neg_acc : 0.47718\n",
      "train/contacts_vel_loss : 0.87042\n",
      "train/smpl_joint_loss : 0.01322\n",
      "train/smpl_mesh_loss : 0.01670\n",
      "train/smpl_joint_consistency_loss : 0.03769\n",
      "train/reconstr_weighted_loss : 0.72195\n",
      "train/loss : 0.72232\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.54920\n",
      "[===========>---------------------------------------] train epoch - 2/200 | batch - 21/93\n",
      "0.540 s per batch | 2 m 55 s elapsed | 7 h 57 m 54 s ETA\n",
      "train/kl_loss : 47.82910\n",
      "train/kl_anneal_weight : 0.02000\n",
      "train/kl_weighted_loss : 0.00038\n",
      "train/trans_loss : 0.00734\n",
      "train/trans_vel_loss : 0.02582\n",
      "train/root_orient_loss : 0.00820\n",
      "train/root_orient_vel_loss : 0.40631\n",
      "train/pose_body_loss : 0.02487\n",
      "train/joints_loss : 0.02562\n",
      "train/joints_vel_loss : 0.13898\n",
      "train/contacts_loss : 0.70125\n",
      "train/contacts_acc : 0.54957\n",
      "train/contacts_pos_acc : 0.73077\n",
      "train/contacts_neg_acc : 0.47492\n",
      "train/contacts_vel_loss : 0.87819\n",
      "train/smpl_joint_loss : 0.01277\n",
      "train/smpl_mesh_loss : 0.01613\n",
      "train/smpl_joint_consistency_loss : 0.03633\n",
      "train/reconstr_weighted_loss : 0.71816\n",
      "train/loss : 0.71854\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.54014\n",
      "[================>----------------------------------] train epoch - 2/200 | batch - 31/93\n",
      "0.552 s per batch | 3 m 18 s elapsed | 8 h 16 m 2 s ETA\n",
      "train/kl_loss : 48.58069\n",
      "train/kl_anneal_weight : 0.02000\n",
      "train/kl_weighted_loss : 0.00039\n",
      "train/trans_loss : 0.00707\n",
      "train/trans_vel_loss : 0.02528\n",
      "train/root_orient_loss : 0.00794\n",
      "train/root_orient_vel_loss : 0.40449\n",
      "train/pose_body_loss : 0.02399\n",
      "train/joints_loss : 0.02485\n",
      "train/joints_vel_loss : 0.13704\n",
      "train/contacts_loss : 0.70124\n",
      "train/contacts_acc : 0.54932\n",
      "train/contacts_pos_acc : 0.73501\n",
      "train/contacts_neg_acc : 0.47282\n",
      "train/contacts_vel_loss : 0.84918\n",
      "train/smpl_joint_loss : 0.01234\n",
      "train/smpl_mesh_loss : 0.01559\n",
      "train/smpl_joint_consistency_loss : 0.03518\n",
      "train/reconstr_weighted_loss : 0.70926\n",
      "train/loss : 0.70965\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.55237\n",
      "[======================>----------------------------] train epoch - 2/200 | batch - 41/93\n",
      "0.556 s per batch | 3 m 40 s elapsed | 8 h 29 m 33 s ETA\n",
      "train/kl_loss : 49.44413\n",
      "train/kl_anneal_weight : 0.02000\n",
      "train/kl_weighted_loss : 0.00040\n",
      "train/trans_loss : 0.00683\n",
      "train/trans_vel_loss : 0.02473\n",
      "train/root_orient_loss : 0.00766\n",
      "train/root_orient_vel_loss : 0.39598\n",
      "train/pose_body_loss : 0.02316\n",
      "train/joints_loss : 0.02410\n",
      "train/joints_vel_loss : 0.13186\n",
      "train/contacts_loss : 0.70000\n",
      "train/contacts_acc : 0.55011\n",
      "train/contacts_pos_acc : 0.73970\n",
      "train/contacts_neg_acc : 0.47204\n",
      "train/contacts_vel_loss : 0.84175\n",
      "train/smpl_joint_loss : 0.01193\n",
      "train/smpl_mesh_loss : 0.01507\n",
      "train/smpl_joint_consistency_loss : 0.03409\n",
      "train/reconstr_weighted_loss : 0.69084\n",
      "train/loss : 0.69124\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.55605\n",
      "[===========================>-----------------------] train epoch - 2/200 | batch - 51/93\n",
      "0.551 s per batch | 4 m 0 s elapsed | 8 h 38 m 14 s ETA\n",
      "train/kl_loss : 50.15666\n",
      "train/kl_anneal_weight : 0.02000\n",
      "train/kl_weighted_loss : 0.00040\n",
      "train/trans_loss : 0.00664\n",
      "train/trans_vel_loss : 0.02434\n",
      "train/root_orient_loss : 0.00745\n",
      "train/root_orient_vel_loss : 0.39197\n",
      "train/pose_body_loss : 0.02244\n",
      "train/joints_loss : 0.02345\n",
      "train/joints_vel_loss : 0.13052\n",
      "train/contacts_loss : 0.70000\n",
      "train/contacts_acc : 0.54951\n",
      "train/contacts_pos_acc : 0.74231\n",
      "train/contacts_neg_acc : 0.47026\n",
      "train/contacts_vel_loss : 0.83424\n",
      "train/smpl_joint_loss : 0.01159\n",
      "train/smpl_mesh_loss : 0.01463\n",
      "train/smpl_joint_consistency_loss : 0.03314\n",
      "train/reconstr_weighted_loss : 0.68150\n",
      "train/loss : 0.68190\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.55065\n",
      "[================================>------------------] train epoch - 2/200 | batch - 61/93\n",
      "0.550 s per batch | 4 m 23 s elapsed | 8 h 48 m 41 s ETA\n",
      "train/kl_loss : 50.88739\n",
      "train/kl_anneal_weight : 0.02000\n",
      "train/kl_weighted_loss : 0.00041\n",
      "train/trans_loss : 0.00642\n",
      "train/trans_vel_loss : 0.02400\n",
      "train/root_orient_loss : 0.00724\n",
      "train/root_orient_vel_loss : 0.38708\n",
      "train/pose_body_loss : 0.02178\n",
      "train/joints_loss : 0.02282\n",
      "train/joints_vel_loss : 0.12862\n",
      "train/contacts_loss : 0.69941\n",
      "train/contacts_acc : 0.54955\n",
      "train/contacts_pos_acc : 0.74569\n",
      "train/contacts_neg_acc : 0.46921\n",
      "train/contacts_vel_loss : 0.82420\n",
      "train/smpl_joint_loss : 0.01123\n",
      "train/smpl_mesh_loss : 0.01419\n",
      "train/smpl_joint_consistency_loss : 0.03220\n",
      "train/reconstr_weighted_loss : 0.67082\n",
      "train/loss : 0.67122\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.55035\n",
      "[======================================>------------] train epoch - 2/200 | batch - 71/93\n",
      "0.547 s per batch | 4 m 43 s elapsed | 8 h 54 m 4 s ETA\n",
      "train/kl_loss : 51.52016\n",
      "train/kl_anneal_weight : 0.02000\n",
      "train/kl_weighted_loss : 0.00041\n",
      "train/trans_loss : 0.00624\n",
      "train/trans_vel_loss : 0.02378\n",
      "train/root_orient_loss : 0.00707\n",
      "train/root_orient_vel_loss : 0.38665\n",
      "train/pose_body_loss : 0.02120\n",
      "train/joints_loss : 0.02226\n",
      "train/joints_vel_loss : 0.12569\n",
      "train/contacts_loss : 0.69886\n",
      "train/contacts_acc : 0.54935\n",
      "train/contacts_pos_acc : 0.74796\n",
      "train/contacts_neg_acc : 0.46795\n",
      "train/contacts_vel_loss : 0.81551\n",
      "train/smpl_joint_loss : 0.01093\n",
      "train/smpl_mesh_loss : 0.01381\n",
      "train/smpl_joint_consistency_loss : 0.03138\n",
      "train/reconstr_weighted_loss : 0.66416\n",
      "train/loss : 0.66457\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.54660\n",
      "[===========================================>-------] train epoch - 2/200 | batch - 81/93\n",
      "0.540 s per batch | 5 m 3 s elapsed | 8 h 58 m 12 s ETA\n",
      "train/kl_loss : 52.16912\n",
      "train/kl_anneal_weight : 0.02000\n",
      "train/kl_weighted_loss : 0.00042\n",
      "train/trans_loss : 0.00607\n",
      "train/trans_vel_loss : 0.02338\n",
      "train/root_orient_loss : 0.00690\n",
      "train/root_orient_vel_loss : 0.38233\n",
      "train/pose_body_loss : 0.02064\n",
      "train/joints_loss : 0.02173\n",
      "train/joints_vel_loss : 0.12462\n",
      "train/contacts_loss : 0.69873\n",
      "train/contacts_acc : 0.54929\n",
      "train/contacts_pos_acc : 0.74868\n",
      "train/contacts_neg_acc : 0.46742\n",
      "train/contacts_vel_loss : 0.80848\n",
      "train/smpl_joint_loss : 0.01065\n",
      "train/smpl_mesh_loss : 0.01344\n",
      "train/smpl_joint_consistency_loss : 0.03060\n",
      "train/reconstr_weighted_loss : 0.65543\n",
      "train/loss : 0.65585\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.54010\n",
      "[================================================>--] train epoch - 2/200 | batch - 91/93\n",
      "0.530 s per batch | 5 m 15 s elapsed | 8 h 49 m 19 s ETA\n",
      "train/kl_loss : 52.85758\n",
      "train/kl_anneal_weight : 0.02000\n",
      "train/kl_weighted_loss : 0.00042\n",
      "train/trans_loss : 0.00591\n",
      "train/trans_vel_loss : 0.02312\n",
      "train/root_orient_loss : 0.00674\n",
      "train/root_orient_vel_loss : 0.38384\n",
      "train/pose_body_loss : 0.02011\n",
      "train/joints_loss : 0.02123\n",
      "train/joints_vel_loss : 0.12318\n",
      "train/contacts_loss : 0.69793\n",
      "train/contacts_acc : 0.54943\n",
      "train/contacts_pos_acc : 0.75041\n",
      "train/contacts_neg_acc : 0.46684\n",
      "train/contacts_vel_loss : 0.79903\n",
      "train/smpl_joint_loss : 0.01037\n",
      "train/smpl_mesh_loss : 0.01309\n",
      "train/smpl_joint_consistency_loss : 0.02986\n",
      "train/reconstr_weighted_loss : 0.65242\n",
      "train/loss : 0.65284\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.52977\n",
      "Best train loss so far! Saving checkpoint...\n",
      "Scheduled sampling current use_gt_p = 1.000000\n",
      "[>--------------------------------------------------] train epoch - 3/200 | batch - 1/93\n",
      "0.491 s per batch | 5 m 18 s elapsed | 8 h 44 m 56 s ETA\n",
      "train/kl_loss : 58.89135\n",
      "train/kl_anneal_weight : 0.04000\n",
      "train/kl_weighted_loss : 0.00094\n",
      "train/trans_loss : 0.00462\n",
      "train/trans_vel_loss : 0.01819\n",
      "train/root_orient_loss : 0.00517\n",
      "train/root_orient_vel_loss : 0.34476\n",
      "train/pose_body_loss : 0.01497\n",
      "train/joints_loss : 0.01646\n",
      "train/joints_vel_loss : 0.08445\n",
      "train/contacts_loss : 0.68923\n",
      "train/contacts_acc : 0.54483\n",
      "train/contacts_pos_acc : 0.77674\n",
      "train/contacts_neg_acc : 0.44434\n",
      "train/contacts_vel_loss : 0.74931\n",
      "train/smpl_joint_loss : 0.00804\n",
      "train/smpl_mesh_loss : 0.00997\n",
      "train/smpl_joint_consistency_loss : 0.02308\n",
      "train/reconstr_weighted_loss : 0.54410\n",
      "train/loss : 0.54504\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.49117\n",
      "[=====>---------------------------------------------] train epoch - 3/200 | batch - 11/93\n",
      "0.437 s per batch | 5 m 28 s elapsed | 8 h 33 m 34 s ETA\n",
      "train/kl_loss : 59.25087\n",
      "train/kl_anneal_weight : 0.04000\n",
      "train/kl_weighted_loss : 0.00095\n",
      "train/trans_loss : 0.00445\n",
      "train/trans_vel_loss : 0.02055\n",
      "train/root_orient_loss : 0.00524\n",
      "train/root_orient_vel_loss : 0.37286\n",
      "train/pose_body_loss : 0.01516\n",
      "train/joints_loss : 0.01643\n",
      "train/joints_vel_loss : 0.10883\n",
      "train/contacts_loss : 0.69520\n",
      "train/contacts_acc : 0.54367\n",
      "train/contacts_pos_acc : 0.76161\n",
      "train/contacts_neg_acc : 0.45343\n",
      "train/contacts_vel_loss : 0.79006\n",
      "train/smpl_joint_loss : 0.00794\n",
      "train/smpl_mesh_loss : 0.01000\n",
      "train/smpl_joint_consistency_loss : 0.02292\n",
      "train/reconstr_weighted_loss : 0.59923\n",
      "train/loss : 0.60017\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.43738\n",
      "[===========>---------------------------------------] train epoch - 3/200 | batch - 21/93\n",
      "0.434 s per batch | 5 m 38 s elapsed | 8 h 23 m 23 s ETA\n",
      "train/kl_loss : 59.84867\n",
      "train/kl_anneal_weight : 0.04000\n",
      "train/kl_weighted_loss : 0.00096\n",
      "train/trans_loss : 0.00437\n",
      "train/trans_vel_loss : 0.02028\n",
      "train/root_orient_loss : 0.00507\n",
      "train/root_orient_vel_loss : 0.36774\n",
      "train/pose_body_loss : 0.01488\n",
      "train/joints_loss : 0.01610\n",
      "train/joints_vel_loss : 0.12306\n",
      "train/contacts_loss : 0.69261\n",
      "train/contacts_acc : 0.54724\n",
      "train/contacts_pos_acc : 0.76247\n",
      "train/contacts_neg_acc : 0.45845\n",
      "train/contacts_vel_loss : 0.77943\n",
      "train/smpl_joint_loss : 0.00777\n",
      "train/smpl_mesh_loss : 0.00981\n",
      "train/smpl_joint_consistency_loss : 0.02249\n",
      "train/reconstr_weighted_loss : 0.60629\n",
      "train/loss : 0.60725\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.43443\n",
      "[================>----------------------------------] train epoch - 3/200 | batch - 31/93\n",
      "0.434 s per batch | 5 m 48 s elapsed | 8 h 14 m 22 s ETA\n",
      "train/kl_loss : 60.51980\n",
      "train/kl_anneal_weight : 0.04000\n",
      "train/kl_weighted_loss : 0.00097\n",
      "train/trans_loss : 0.00427\n",
      "train/trans_vel_loss : 0.01984\n",
      "train/root_orient_loss : 0.00495\n",
      "train/root_orient_vel_loss : 0.35961\n",
      "train/pose_body_loss : 0.01455\n",
      "train/joints_loss : 0.01578\n",
      "train/joints_vel_loss : 0.11789\n",
      "train/contacts_loss : 0.69228\n",
      "train/contacts_acc : 0.54781\n",
      "train/contacts_pos_acc : 0.76284\n",
      "train/contacts_neg_acc : 0.45926\n",
      "train/contacts_vel_loss : 0.76056\n",
      "train/smpl_joint_loss : 0.00759\n",
      "train/smpl_mesh_loss : 0.00959\n",
      "train/smpl_joint_consistency_loss : 0.02201\n",
      "train/reconstr_weighted_loss : 0.59063\n",
      "train/loss : 0.59159\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.43424\n",
      "[======================>----------------------------] train epoch - 3/200 | batch - 41/93\n",
      "0.434 s per batch | 5 m 58 s elapsed | 8 h 5 m 52 s ETA\n",
      "train/kl_loss : 60.87560\n",
      "train/kl_anneal_weight : 0.04000\n",
      "train/kl_weighted_loss : 0.00097\n",
      "train/trans_loss : 0.00420\n",
      "train/trans_vel_loss : 0.01967\n",
      "train/root_orient_loss : 0.00487\n",
      "train/root_orient_vel_loss : 0.35695\n",
      "train/pose_body_loss : 0.01431\n",
      "train/joints_loss : 0.01553\n",
      "train/joints_vel_loss : 0.11521\n",
      "train/contacts_loss : 0.69271\n",
      "train/contacts_acc : 0.54698\n",
      "train/contacts_pos_acc : 0.76018\n",
      "train/contacts_neg_acc : 0.45901\n",
      "train/contacts_vel_loss : 0.75575\n",
      "train/smpl_joint_loss : 0.00747\n",
      "train/smpl_mesh_loss : 0.00943\n",
      "train/smpl_joint_consistency_loss : 0.02165\n",
      "train/reconstr_weighted_loss : 0.58379\n",
      "train/loss : 0.58476\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.43372\n",
      "[===========================>-----------------------] train epoch - 3/200 | batch - 51/93\n",
      "0.435 s per batch | 6 m 8 s elapsed | 7 h 58 m 23 s ETA\n",
      "train/kl_loss : 61.43372\n",
      "train/kl_anneal_weight : 0.04000\n",
      "train/kl_weighted_loss : 0.00098\n",
      "train/trans_loss : 0.00411\n",
      "train/trans_vel_loss : 0.01995\n",
      "train/root_orient_loss : 0.00479\n",
      "train/root_orient_vel_loss : 0.35878\n",
      "train/pose_body_loss : 0.01405\n",
      "train/joints_loss : 0.01526\n",
      "train/joints_vel_loss : 0.11477\n",
      "train/contacts_loss : 0.69172\n",
      "train/contacts_acc : 0.54805\n",
      "train/contacts_pos_acc : 0.76071\n",
      "train/contacts_neg_acc : 0.46049\n",
      "train/contacts_vel_loss : 0.75152\n",
      "train/smpl_joint_loss : 0.00732\n",
      "train/smpl_mesh_loss : 0.00924\n",
      "train/smpl_joint_consistency_loss : 0.02123\n",
      "train/reconstr_weighted_loss : 0.58391\n",
      "train/loss : 0.58490\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.43470\n",
      "[================================>------------------] train epoch - 3/200 | batch - 61/93\n",
      "0.435 s per batch | 6 m 19 s elapsed | 7 h 51 m 22 s ETA\n",
      "train/kl_loss : 61.93505\n",
      "train/kl_anneal_weight : 0.04000\n",
      "train/kl_weighted_loss : 0.00099\n",
      "train/trans_loss : 0.00405\n",
      "train/trans_vel_loss : 0.01983\n",
      "train/root_orient_loss : 0.00470\n",
      "train/root_orient_vel_loss : 0.35434\n",
      "train/pose_body_loss : 0.01380\n",
      "train/joints_loss : 0.01500\n",
      "train/joints_vel_loss : 0.11155\n",
      "train/contacts_loss : 0.69089\n",
      "train/contacts_acc : 0.54848\n",
      "train/contacts_pos_acc : 0.76064\n",
      "train/contacts_neg_acc : 0.46083\n",
      "train/contacts_vel_loss : 0.73683\n",
      "train/smpl_joint_loss : 0.00719\n",
      "train/smpl_mesh_loss : 0.00907\n",
      "train/smpl_joint_consistency_loss : 0.02086\n",
      "train/reconstr_weighted_loss : 0.57467\n",
      "train/loss : 0.57566\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.43493\n",
      "[======================================>------------] train epoch - 3/200 | batch - 71/93\n",
      "0.434 s per batch | 6 m 29 s elapsed | 7 h 44 m 51 s ETA\n",
      "train/kl_loss : 62.36480\n",
      "train/kl_anneal_weight : 0.04000\n",
      "train/kl_weighted_loss : 0.00100\n",
      "train/trans_loss : 0.00397\n",
      "train/trans_vel_loss : 0.01961\n",
      "train/root_orient_loss : 0.00462\n",
      "train/root_orient_vel_loss : 0.35328\n",
      "train/pose_body_loss : 0.01358\n",
      "train/joints_loss : 0.01476\n",
      "train/joints_vel_loss : 0.11724\n",
      "train/contacts_loss : 0.69070\n",
      "train/contacts_acc : 0.54847\n",
      "train/contacts_pos_acc : 0.75936\n",
      "train/contacts_neg_acc : 0.46154\n",
      "train/contacts_vel_loss : 0.73366\n",
      "train/smpl_joint_loss : 0.00705\n",
      "train/smpl_mesh_loss : 0.00891\n",
      "train/smpl_joint_consistency_loss : 0.02051\n",
      "train/reconstr_weighted_loss : 0.57778\n",
      "train/loss : 0.57877\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.43356\n",
      "[===========================================>-------] train epoch - 3/200 | batch - 81/93\n",
      "0.433 s per batch | 6 m 39 s elapsed | 7 h 38 m 43 s ETA\n",
      "train/kl_loss : 62.73918\n",
      "train/kl_anneal_weight : 0.04000\n",
      "train/kl_weighted_loss : 0.00100\n",
      "train/trans_loss : 0.00390\n",
      "train/trans_vel_loss : 0.01939\n",
      "train/root_orient_loss : 0.00455\n",
      "train/root_orient_vel_loss : 0.35128\n",
      "train/pose_body_loss : 0.01335\n",
      "train/joints_loss : 0.01453\n",
      "train/joints_vel_loss : 0.11773\n",
      "train/contacts_loss : 0.69030\n",
      "train/contacts_acc : 0.54864\n",
      "train/contacts_pos_acc : 0.75807\n",
      "train/contacts_neg_acc : 0.46220\n",
      "train/contacts_vel_loss : 0.73075\n",
      "train/smpl_joint_loss : 0.00693\n",
      "train/smpl_mesh_loss : 0.00876\n",
      "train/smpl_joint_consistency_loss : 0.02017\n",
      "train/reconstr_weighted_loss : 0.57479\n",
      "train/loss : 0.57580\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.43350\n",
      "[================================================>--] train epoch - 3/200 | batch - 91/93\n",
      "0.433 s per batch | 6 m 49 s elapsed | 7 h 32 m 52 s ETA\n",
      "train/kl_loss : 63.17514\n",
      "train/kl_anneal_weight : 0.04000\n",
      "train/kl_weighted_loss : 0.00101\n",
      "train/trans_loss : 0.00383\n",
      "train/trans_vel_loss : 0.01930\n",
      "train/root_orient_loss : 0.00447\n",
      "train/root_orient_vel_loss : 0.35102\n",
      "train/pose_body_loss : 0.01315\n",
      "train/joints_loss : 0.01432\n",
      "train/joints_vel_loss : 0.11576\n",
      "train/contacts_loss : 0.68948\n",
      "train/contacts_acc : 0.54935\n",
      "train/contacts_pos_acc : 0.75883\n",
      "train/contacts_neg_acc : 0.46295\n",
      "train/contacts_vel_loss : 0.72785\n",
      "train/smpl_joint_loss : 0.00682\n",
      "train/smpl_mesh_loss : 0.00861\n",
      "train/smpl_joint_consistency_loss : 0.01985\n",
      "train/reconstr_weighted_loss : 0.57130\n",
      "train/loss : 0.57231\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.43270\n",
      "Best train loss so far! Saving checkpoint...\n",
      "[============>--------------------------------------] val epoch - 3/200 | batch - 1/4\n",
      "0.192 s per batch | 6 m 51 s elapsed | 11 h 18 m 42 s ETA\n",
      "val/kl_loss : 75.88914\n",
      "val/kl_anneal_weight : 0.04000\n",
      "val/kl_weighted_loss : 0.00121\n",
      "val/trans_loss : 0.00329\n",
      "val/trans_vel_loss : 0.01809\n",
      "val/root_orient_loss : 0.00391\n",
      "val/root_orient_vel_loss : 0.25133\n",
      "val/pose_body_loss : 0.01172\n",
      "val/joints_loss : 0.01314\n",
      "val/joints_vel_loss : 0.05361\n",
      "val/contacts_loss : 0.78054\n",
      "val/contacts_acc : 0.50400\n",
      "val/contacts_pos_acc : 0.82005\n",
      "val/contacts_neg_acc : 0.37400\n",
      "val/contacts_vel_loss : 0.89169\n",
      "val/smpl_joint_loss : 0.00593\n",
      "val/smpl_mesh_loss : 0.00738\n",
      "val/smpl_joint_consistency_loss : 0.01782\n",
      "val/reconstr_weighted_loss : 0.40295\n",
      "val/loss : 0.40417\n",
      "val/time_per_batch : 0.19248\n",
      "Best val loss so far! Saving checkpoint...\n",
      "Scheduled sampling current use_gt_p = 0.875000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\workspace\\Motion\\humor_local\\humor\\utils\\..\\utils\\transforms.py:26: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Cross.cpp:66.)\n",
      "  world2aligned_axis = torch.cross(body_right, x_axis.expand_as(body_right))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>--------------------------------------------------] train epoch - 4/200 | batch - 1/93\n",
      "0.557 s per batch | 6 m 55 s elapsed | 7 h 34 m 11 s ETA\n",
      "train/kl_loss : 67.69653\n",
      "train/kl_anneal_weight : 0.06000\n",
      "train/kl_weighted_loss : 0.00162\n",
      "train/trans_loss : 0.00342\n",
      "train/trans_vel_loss : 0.02055\n",
      "train/root_orient_loss : 0.00406\n",
      "train/root_orient_vel_loss : 0.34778\n",
      "train/pose_body_loss : 0.01127\n",
      "train/joints_loss : 0.01231\n",
      "train/joints_vel_loss : 0.09822\n",
      "train/contacts_loss : 0.69181\n",
      "train/contacts_acc : 0.54494\n",
      "train/contacts_pos_acc : 0.75234\n",
      "train/contacts_neg_acc : 0.46012\n",
      "train/contacts_vel_loss : 0.67227\n",
      "train/smpl_joint_loss : 0.00605\n",
      "train/smpl_mesh_loss : 0.00759\n",
      "train/smpl_joint_consistency_loss : 0.01722\n",
      "train/reconstr_weighted_loss : 0.54210\n",
      "train/loss : 0.54373\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.55682\n",
      "[=====>---------------------------------------------] train epoch - 4/200 | batch - 11/93\n",
      "0.485 s per batch | 7 m 5 s elapsed | 7 h 29 m 19 s ETA\n",
      "train/kl_loss : 68.15884\n",
      "train/kl_anneal_weight : 0.06000\n",
      "train/kl_weighted_loss : 0.00164\n",
      "train/trans_loss : 0.00379\n",
      "train/trans_vel_loss : 0.02337\n",
      "train/root_orient_loss : 0.00448\n",
      "train/root_orient_vel_loss : 0.35286\n",
      "train/pose_body_loss : 0.01414\n",
      "train/joints_loss : 0.01580\n",
      "train/joints_vel_loss : 0.11383\n",
      "train/contacts_loss : 0.68495\n",
      "train/contacts_acc : 0.55233\n",
      "train/contacts_pos_acc : 0.76937\n",
      "train/contacts_neg_acc : 0.46635\n",
      "train/contacts_vel_loss : 0.71271\n",
      "train/smpl_joint_loss : 0.00706\n",
      "train/smpl_mesh_loss : 0.00892\n",
      "train/smpl_joint_consistency_loss : 0.02138\n",
      "train/reconstr_weighted_loss : 0.57960\n",
      "train/loss : 0.58124\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.48481\n",
      "[===========>---------------------------------------] train epoch - 4/200 | batch - 21/93\n",
      "0.480 s per batch | 7 m 16 s elapsed | 7 h 24 m 50 s ETA\n",
      "train/kl_loss : 68.32355\n",
      "train/kl_anneal_weight : 0.06000\n",
      "train/kl_weighted_loss : 0.00164\n",
      "train/trans_loss : 0.00369\n",
      "train/trans_vel_loss : 0.02192\n",
      "train/root_orient_loss : 0.00424\n",
      "train/root_orient_vel_loss : 0.35876\n",
      "train/pose_body_loss : 0.01317\n",
      "train/joints_loss : 0.01461\n",
      "train/joints_vel_loss : 0.12494\n",
      "train/contacts_loss : 0.68626\n",
      "train/contacts_acc : 0.55134\n",
      "train/contacts_pos_acc : 0.76389\n",
      "train/contacts_neg_acc : 0.46583\n",
      "train/contacts_vel_loss : 0.70264\n",
      "train/smpl_joint_loss : 0.00672\n",
      "train/smpl_mesh_loss : 0.00849\n",
      "train/smpl_joint_consistency_loss : 0.01995\n",
      "train/reconstr_weighted_loss : 0.59038\n",
      "train/loss : 0.59202\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.48022\n",
      "[================>----------------------------------] train epoch - 4/200 | batch - 31/93\n",
      "0.479 s per batch | 7 m 26 s elapsed | 7 h 20 m 30 s ETA\n",
      "train/kl_loss : 68.81851\n",
      "train/kl_anneal_weight : 0.06000\n",
      "train/kl_weighted_loss : 0.00165\n",
      "train/trans_loss : 0.00361\n",
      "train/trans_vel_loss : 0.02178\n",
      "train/root_orient_loss : 0.00417\n",
      "train/root_orient_vel_loss : 0.35389\n",
      "train/pose_body_loss : 0.01295\n",
      "train/joints_loss : 0.01436\n",
      "train/joints_vel_loss : 0.12010\n",
      "train/contacts_loss : 0.68742\n",
      "train/contacts_acc : 0.54953\n",
      "train/contacts_pos_acc : 0.76189\n",
      "train/contacts_neg_acc : 0.46357\n",
      "train/contacts_vel_loss : 0.70209\n",
      "train/smpl_joint_loss : 0.00657\n",
      "train/smpl_mesh_loss : 0.00832\n",
      "train/smpl_joint_consistency_loss : 0.01956\n",
      "train/reconstr_weighted_loss : 0.57921\n",
      "train/loss : 0.58086\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.47948\n",
      "[======================>----------------------------] train epoch - 4/200 | batch - 41/93\n",
      "0.479 s per batch | 7 m 36 s elapsed | 7 h 16 m 27 s ETA\n",
      "train/kl_loss : 69.15130\n",
      "train/kl_anneal_weight : 0.06000\n",
      "train/kl_weighted_loss : 0.00166\n",
      "train/trans_loss : 0.00356\n",
      "train/trans_vel_loss : 0.02171\n",
      "train/root_orient_loss : 0.00408\n",
      "train/root_orient_vel_loss : 0.36115\n",
      "train/pose_body_loss : 0.01260\n",
      "train/joints_loss : 0.01396\n",
      "train/joints_vel_loss : 0.12285\n",
      "train/contacts_loss : 0.68648\n",
      "train/contacts_acc : 0.54943\n",
      "train/contacts_pos_acc : 0.76159\n",
      "train/contacts_neg_acc : 0.46262\n",
      "train/contacts_vel_loss : 0.69137\n",
      "train/smpl_joint_loss : 0.00644\n",
      "train/smpl_mesh_loss : 0.00815\n",
      "train/smpl_joint_consistency_loss : 0.01907\n",
      "train/reconstr_weighted_loss : 0.58735\n",
      "train/loss : 0.58901\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.47945\n",
      "[===========================>-----------------------] train epoch - 4/200 | batch - 51/93\n",
      "0.481 s per batch | 7 m 47 s elapsed | 7 h 12 m 52 s ETA\n",
      "train/kl_loss : 69.53719\n",
      "train/kl_anneal_weight : 0.06000\n",
      "train/kl_weighted_loss : 0.00167\n",
      "train/trans_loss : 0.00350\n",
      "train/trans_vel_loss : 0.02153\n",
      "train/root_orient_loss : 0.00404\n",
      "train/root_orient_vel_loss : 0.35792\n",
      "train/pose_body_loss : 0.01247\n",
      "train/joints_loss : 0.01383\n",
      "train/joints_vel_loss : 0.12145\n",
      "train/contacts_loss : 0.68530\n",
      "train/contacts_acc : 0.55015\n",
      "train/contacts_pos_acc : 0.76225\n",
      "train/contacts_neg_acc : 0.46343\n",
      "train/contacts_vel_loss : 0.67841\n",
      "train/smpl_joint_loss : 0.00636\n",
      "train/smpl_mesh_loss : 0.00804\n",
      "train/smpl_joint_consistency_loss : 0.01887\n",
      "train/reconstr_weighted_loss : 0.58163\n",
      "train/loss : 0.58330\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.48064\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m     Logger.log(\u001b[33m'\u001b[39m\u001b[33mResetting min_val_loss and min_train_loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     34\u001b[39m     min_val_loss = min_train_loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_start_t\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# zero the gradients\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\newProgramFiles\\tool\\anaconda\\envs\\CS280\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    628\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    633\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\newProgramFiles\\tool\\anaconda\\envs\\CS280\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    672\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    674\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    675\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\newProgramFiles\\tool\\anaconda\\envs\\CS280\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\newProgramFiles\\tool\\anaconda\\envs\\CS280\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\workspace\\Motion\\humor_local\\humor\\utils\\..\\datasets\\amass_discrete_dataset.py:257\u001b[39m, in \u001b[36mAmassDiscreteDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    254\u001b[39m pose_body_vel = data[\u001b[33m'\u001b[39m\u001b[33mpose_body_vel\u001b[39m\u001b[33m'\u001b[39m][sample_frame_inds]\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# Joints\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m joints = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mjoints\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[sample_frame_inds]\n\u001b[32m    258\u001b[39m verts = data[\u001b[33m'\u001b[39m\u001b[33mmojo_verts\u001b[39m\u001b[33m'\u001b[39m][sample_frame_inds]\n\u001b[32m    259\u001b[39m num_verts = verts.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\newProgramFiles\\tool\\anaconda\\envs\\CS280\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:255\u001b[39m, in \u001b[36mNpzFile.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28mbytes\u001b[39m.close()\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m magic == \u001b[38;5;28mformat\u001b[39m.MAGIC_PREFIX:\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m     \u001b[38;5;28mbytes\u001b[39m = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzip\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m.read_array(\u001b[38;5;28mbytes\u001b[39m,\n\u001b[32m    257\u001b[39m                              allow_pickle=\u001b[38;5;28mself\u001b[39m.allow_pickle,\n\u001b[32m    258\u001b[39m                              pickle_kwargs=\u001b[38;5;28mself\u001b[39m.pickle_kwargs,\n\u001b[32m    259\u001b[39m                              max_header_size=\u001b[38;5;28mself\u001b[39m.max_header_size)\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\newProgramFiles\\tool\\anaconda\\envs\\CS280\\Lib\\zipfile.py:1587\u001b[39m, in \u001b[36mZipFile.open\u001b[39m\u001b[34m(self, name, mode, pwd, force_zip64)\u001b[39m\n\u001b[32m   1584\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fheader[_FH_SIGNATURE] != stringFileHeader:\n\u001b[32m   1585\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[33m\"\u001b[39m\u001b[33mBad magic number for file header\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1587\u001b[39m fname = \u001b[43mzef_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfheader\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_FH_FILENAME_LENGTH\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fheader[_FH_EXTRA_FIELD_LENGTH]:\n\u001b[32m   1589\u001b[39m     zef_file.read(fheader[_FH_EXTRA_FIELD_LENGTH])\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\newProgramFiles\\tool\\anaconda\\envs\\CS280\\Lib\\zipfile.py:787\u001b[39m, in \u001b[36m_SharedFile.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28mself\u001b[39m._file.seek(\u001b[38;5;28mself\u001b[39m._pos)\n\u001b[32m    786\u001b[39m data = \u001b[38;5;28mself\u001b[39m._file.read(n)\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m \u001b[38;5;28mself\u001b[39m._pos = \u001b[38;5;28mself\u001b[39m._file.tell()\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# main training loop\n",
    "train_start_t = time.time()\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # train\n",
    "    stat_tracker.reset()\n",
    "    batch_start_t = None\n",
    "    reset_loss_track = train_dataset.pre_batch(epoch=epoch)\n",
    "    # see which phase we're in \n",
    "    sched_samp_gt_p = 1.0 # supervised\n",
    "    if use_sched_samp:\n",
    "        if epoch >= args.sched_samp_start and epoch < args.sched_samp_end:\n",
    "            frac = (epoch - args.sched_samp_start) / (args.sched_samp_end - args.sched_samp_start)\n",
    "            sched_samp_gt_p = 1.0*(1.0 - frac)\n",
    "        elif epoch >= args.sched_samp_end:\n",
    "            # autoregressive\n",
    "            sched_samp_gt_p = 0.0\n",
    "        Logger.log('Scheduled sampling current use_gt_p = %f' % (sched_samp_gt_p))\n",
    "\n",
    "        if epoch == args.sched_samp_end:\n",
    "            # the loss will naturally go up when using own rollouts\n",
    "            reset_loss_track = True\n",
    "\n",
    "        if args_obj.loss_dict['kl_loss_cycle_len'] > 0:\n",
    "            # if we're cycling, only want to save results when using full ELBO\n",
    "            if (epoch % args_obj.loss_dict['kl_loss_cycle_len']) > (args_obj.loss_dict['kl_loss_cycle_len'] // 2):\n",
    "                # have reached second half of a cycle\n",
    "                reset_loss_track = True\n",
    "\n",
    "    if reset_loss_track:\n",
    "        Logger.log('Resetting min_val_loss and min_train_loss')\n",
    "        min_val_loss = min_train_loss = float('inf')\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        batch_start_t = time.time()\n",
    "\n",
    "        try:\n",
    "            # zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # run model\n",
    "            loss, stats_dict = step(model, loss_func, data, train_dataset, device, epoch, mode='train', use_gt_p=sched_samp_gt_p)\n",
    "            if torch.isnan(loss).item():\n",
    "                Logger.log('WARNING: NaN loss. Skipping to next data...')\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            # backprop and step\n",
    "            loss.backward()\n",
    "            # check gradients\n",
    "            parameters = [p for p in model.parameters() if p.grad is not None]\n",
    "            total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), 2.0).to(device) for p in parameters]), 2.0)\n",
    "            if torch.isnan(total_norm):\n",
    "                Logger.log('WARNING: NaN gradients. Skipping to next data...')\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            optimizer.step()\n",
    "        except (RuntimeError, AssertionError) as e:\n",
    "            if epoch > 0:\n",
    "                # to catch bad dynamics, but keep training\n",
    "                Logger.log('WARNING: caught an exception during forward or backward pass. Skipping to next data...')\n",
    "                Logger.log(e)\n",
    "                traceback.print_exc()\n",
    "                reset_loss_track = train_dataset.pre_batch(epoch=epoch)\n",
    "                if reset_loss_track:\n",
    "                    Logger.log('Resetting min_val_loss and min_train_loss')\n",
    "                    min_val_loss = min_train_loss = float('inf')\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "        # collect stats\n",
    "        batch_elapsed_t = time.time() - batch_start_t\n",
    "        total_elapsed_t = time.time() - train_start_t\n",
    "        stats_dict['loss'] = loss\n",
    "        for param_group in optimizer.param_groups:\n",
    "            stats_dict['lr'] = torch.Tensor([param_group['lr']])[0]\n",
    "        stats_dict['time_per_batch'] = torch.Tensor([batch_elapsed_t])[0]\n",
    "\n",
    "        last_batch = (i==(len(train_loader)-1))\n",
    "        stat_tracker.update(stats_dict, tag='train', save_tf=last_batch)\n",
    "        if i % args.print_every == 0:\n",
    "            stat_tracker.print(i, len(train_loader),\n",
    "                            epoch, args.epochs,\n",
    "                            total_elapsed_time=total_elapsed_t,\n",
    "                            tag='train')\n",
    "\n",
    "        reset_loss_track = train_dataset.pre_batch(epoch=epoch)\n",
    "        if reset_loss_track:\n",
    "            Logger.log('Resetting min_val_loss and min_train_loss')\n",
    "            min_val_loss = min_train_loss = float('inf')\n",
    "\n",
    "    # save if desired\n",
    "    if epoch % args.save_every == 0:\n",
    "        Logger.log('Saving checkpoint...')\n",
    "        save_file = os.path.join(ckpts_path, 'epoch_%08d_model.pth' % (epoch))\n",
    "        save_state(save_file, model, optimizer, cur_epoch=epoch, min_val_loss=min_val_loss, min_train_loss=min_train_loss, ignore_keys=model.ignore_keys)\n",
    "\n",
    "    # check if it's the best train model so far\n",
    "    mean_train_loss = stat_tracker.meter_dict['train/loss'].avg\n",
    "    if mean_train_loss < min_train_loss:\n",
    "        min_train_loss = mean_train_loss\n",
    "        Logger.log('Best train loss so far! Saving checkpoint...')\n",
    "        save_file = os.path.join(ckpts_path, 'best_train_model.pth')\n",
    "        save_state(save_file, model, optimizer, cur_epoch=epoch, min_val_loss=min_val_loss, min_train_loss=min_train_loss, ignore_keys=model.ignore_keys)\n",
    "\n",
    "    # validate\n",
    "    if epoch % args.val_every == 0:\n",
    "        with torch.no_grad():\n",
    "            # run on validation data\n",
    "            model.eval()\n",
    "\n",
    "            stat_tracker.reset()\n",
    "            for i, data in enumerate(val_loader):\n",
    "                # print(i)\n",
    "                batch_start_t = time.time()\n",
    "                # run model\n",
    "                loss, stats_dict = step(model, loss_func, data, val_dataset, device, epoch, mode='test', use_gt_p=sched_samp_gt_p)\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    Logger.log('WARNING: NaN loss on VALIDATION. Skipping to next data...')\n",
    "                    continue\n",
    "\n",
    "                # collect stats\n",
    "                batch_elapsed_t = time.time() - batch_start_t\n",
    "                total_elapsed_t = time.time() - train_start_t\n",
    "                stats_dict['loss'] = loss\n",
    "                stats_dict['time_per_batch'] = torch.Tensor([batch_elapsed_t])[0]\n",
    "\n",
    "                stat_tracker.update(stats_dict, tag='val', save_tf=(i==(len(val_loader)-1)), increment_step=False)\n",
    "\n",
    "                if i % args.print_every == 0:\n",
    "                    stat_tracker.print(i, len(val_loader),\n",
    "                                    epoch, args.epochs,\n",
    "                                    total_elapsed_time=total_elapsed_t,\n",
    "                                    tag='val')\n",
    "\n",
    "            # check if it's the best model so far\n",
    "            mean_val_loss = stat_tracker.meter_dict['val/loss'].avg\n",
    "            if mean_val_loss < min_val_loss:\n",
    "                min_val_loss = mean_val_loss\n",
    "                Logger.log('Best val loss so far! Saving checkpoint...')\n",
    "                save_file = os.path.join(ckpts_path, 'best_model.pth')\n",
    "                save_state(save_file, model, optimizer, cur_epoch=epoch, min_val_loss=min_val_loss, min_train_loss=min_train_loss, ignore_keys=model.ignore_keys)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "Logger.log('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(args_obj, config_file):\n",
    "\n",
    "    # # set up output\n",
    "    # args = args_obj.base\n",
    "    # mkdir(args.out)\n",
    "\n",
    "    # # create logging system\n",
    "    # train_log_path = os.path.join(args.out, 'train.log')\n",
    "    # Logger.init(train_log_path)\n",
    "\n",
    "    # # save arguments used\n",
    "    # Logger.log('Base args: ' + str(args))\n",
    "    # Logger.log('Model args: ' + str(args_obj.model))\n",
    "    # Logger.log('Dataset args: ' + str(args_obj.dataset))\n",
    "    # Logger.log('Loss args: ' + str(args_obj.loss))\n",
    "\n",
    "    # # save training script/model/dataset used\n",
    "    # train_scripts_path = os.path.join(args.out, 'train_scripts')\n",
    "    # mkdir(train_scripts_path)\n",
    "    # pkg_root = os.path.join(cur_file_path, '..')\n",
    "    # dataset_file = class_name_to_file_name(args.dataset)\n",
    "    # dataset_file_path = os.path.join(pkg_root, 'datasets/' + dataset_file + '.py')\n",
    "    # model_file = class_name_to_file_name(args.model)\n",
    "    # loss_file = class_name_to_file_name(args.loss)\n",
    "    # model_file_path = os.path.join(pkg_root, 'models/' + model_file + '.py')\n",
    "    # train_file_path = os.path.join(pkg_root, 'train/train_humor.py')\n",
    "    # cp_files(train_scripts_path, [train_file_path, model_file_path, dataset_file_path, config_file])\n",
    "\n",
    "    # # load model class and instantiate\n",
    "    # model_class = importlib.import_module('models.' + model_file)\n",
    "    # Model = getattr(model_class, args.model)\n",
    "    # model = Model(**args_obj.model_dict,\n",
    "    #                 model_smpl_batch_size=args.batch_size) # assumes model is HumorModel\n",
    "\n",
    "    # load loss class and instantiate\n",
    "    # loss_class = importlib.import_module('losses.' + loss_file)\n",
    "    # Loss = getattr(loss_class, args.loss)\n",
    "    # loss_func = Loss(**args_obj.loss_dict,\n",
    "    #                  smpl_batch_size=args.batch_size*args_obj.dataset.sample_num_frames) # assumes loss is HumorLoss\n",
    "\n",
    "    # device = get_device(args.gpu)\n",
    "    # model.to(device)\n",
    "    # loss_func.to(device)\n",
    "\n",
    "    # print(model)\n",
    "\n",
    "    # # count params in model\n",
    "    # model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    # params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    # Logger.log('Num model params: ' + str(params))\n",
    "\n",
    "    # # freeze params in loss\n",
    "    # for param in loss_func.parameters():\n",
    "    #     param.requires_grad = False\n",
    "\n",
    "    # # optimizer\n",
    "    # betas = (args.beta1, args.beta2)\n",
    "    # if args.use_adam:\n",
    "    #     optimizer = optim.Adam(model.parameters(),\n",
    "    #                             lr=args.lr,\n",
    "    #                             betas=betas,\n",
    "    #                             eps=args.eps,\n",
    "    #                             weight_decay=args.decay)\n",
    "    # else:\n",
    "    #     optimizer = optim.Adamax(model.parameters(),\n",
    "    #                             lr=args.lr,\n",
    "    #                             betas=betas,\n",
    "    #                             eps=args.eps,\n",
    "    #                             weight_decay=args.decay)\n",
    "\n",
    "    # # load in pretrained weights/optimizer state if given\n",
    "    # start_epoch = 0\n",
    "    # min_val_loss = min_train_loss = float('inf')\n",
    "    # if args.ckpt is not None:\n",
    "    #     load_optim = optimizer if args.load_optim else None\n",
    "    #     start_epoch, min_val_loss, min_train_loss = load_state(args.ckpt, model, optimizer=load_optim, map_location=device, ignore_keys=model.ignore_keys)\n",
    "    #     start_epoch += 1\n",
    "    #     Logger.log('Resuming from saved checkpoint at epoch idx %d with min val loss %.6f...' % (start_epoch, min_val_loss))\n",
    "    #     if not args.load_optim:\n",
    "    #         Logger.log('Not loading optimizer state as desired...')\n",
    "    #         Logger.log('WARNING: Also resetting min_val_loss and epoch count!')\n",
    "    #         min_val_loss = float('inf')\n",
    "    #         start_epoch = 0\n",
    "\n",
    "    # # initialize LR scheduler\n",
    "    # scheduler = MultiStepLR(optimizer, milestones=args.sched_milestones, gamma=args.sched_decay)\n",
    "\n",
    "    # # intialize schedule sampling if desired\n",
    "    # use_sched_samp = False\n",
    "    # if args.sched_samp_start is not None and args.sched_samp_end is not None:\n",
    "    #     if args.sched_samp_start >= 0 and args.sched_samp_end >= args.sched_samp_start:\n",
    "    #         Logger.log('Using scheduled sampling starting at epoch %d and ending at epoch %d!' % (args.sched_samp_start, args.sched_samp_end))\n",
    "    #         use_sched_samp = True\n",
    "    #     else:\n",
    "    #         Logger.log('Could not use scheduled sampling with given start and end!')\n",
    "\n",
    "    # load dataset class and instantiate training and validation set\n",
    "    # Dataset = getattr(importlib.import_module('datasets.' + dataset_file), args.dataset)\n",
    "    # train_dataset = Dataset(split='train', **args_obj.dataset_dict)\n",
    "    # val_dataset = Dataset(split='val', **args_obj.dataset_dict)\n",
    "    # # create loaders\n",
    "    # train_loader = DataLoader(train_dataset, \n",
    "    #                           batch_size=args.batch_size,\n",
    "    #                           shuffle=True,\n",
    "    #                           num_workers=NUM_WORKERS,\n",
    "    #                           pin_memory=True,\n",
    "    #                           worker_init_fn=lambda _: np.random.seed()) # get around pytorch RNG seed bug\n",
    "    # val_loader = DataLoader(val_dataset, \n",
    "    #                         batch_size=args.batch_size,\n",
    "    #                         shuffle=False, \n",
    "    #                         num_workers=NUM_WORKERS,\n",
    "    #                         pin_memory=True,\n",
    "    #                         worker_init_fn=lambda _: np.random.seed())\n",
    "\n",
    "    # stats tracker\n",
    "    # tensorboard_path = os.path.join(args.out, 'train_tensorboard')\n",
    "    # mkdir(tensorboard_path)\n",
    "    # stat_tracker = StatTracker(tensorboard_path)\n",
    "\n",
    "    # # checkpoints saving\n",
    "    # ckpts_path = os.path.join(args.out, 'checkpoints')\n",
    "    # mkdir(ckpts_path)\n",
    "\n",
    "    # if use_sched_samp:\n",
    "    #     train_dataset.return_global = True\n",
    "    #     val_dataset.return_global = True\n",
    "\n",
    "    # # main training loop\n",
    "    # train_start_t = time.time()\n",
    "    # for epoch in range(start_epoch, args.epochs):\n",
    "\n",
    "    #     model.train()\n",
    "\n",
    "    #     # train\n",
    "    #     stat_tracker.reset()\n",
    "    #     batch_start_t = None\n",
    "    #     reset_loss_track = train_dataset.pre_batch(epoch=epoch)\n",
    "    #     # see which phase we're in \n",
    "    #     sched_samp_gt_p = 1.0 # supervised\n",
    "    #     if use_sched_samp:\n",
    "    #         if epoch >= args.sched_samp_start and epoch < args.sched_samp_end:\n",
    "    #             frac = (epoch - args.sched_samp_start) / (args.sched_samp_end - args.sched_samp_start)\n",
    "    #             sched_samp_gt_p = 1.0*(1.0 - frac)\n",
    "    #         elif epoch >= args.sched_samp_end:\n",
    "    #             # autoregressive\n",
    "    #             sched_samp_gt_p = 0.0\n",
    "    #         Logger.log('Scheduled sampling current use_gt_p = %f' % (sched_samp_gt_p))\n",
    "\n",
    "    #         if epoch == args.sched_samp_end:\n",
    "    #             # the loss will naturally go up when using own rollouts\n",
    "    #             reset_loss_track = True\n",
    "\n",
    "    #         if args_obj.loss_dict['kl_loss_cycle_len'] > 0:\n",
    "    #             # if we're cycling, only want to save results when using full ELBO\n",
    "    #             if (epoch % args_obj.loss_dict['kl_loss_cycle_len']) > (args_obj.loss_dict['kl_loss_cycle_len'] // 2):\n",
    "    #                 # have reached second half of a cycle\n",
    "    #                 reset_loss_track = True\n",
    "\n",
    "    #     if reset_loss_track:\n",
    "    #         Logger.log('Resetting min_val_loss and min_train_loss')\n",
    "    #         min_val_loss = min_train_loss = float('inf')\n",
    "\n",
    "    #     for i, data in enumerate(train_loader):\n",
    "    #         batch_start_t = time.time()\n",
    "\n",
    "    #         try:\n",
    "    #             # zero the gradients\n",
    "    #             optimizer.zero_grad()\n",
    "    #             # run model\n",
    "    #             loss, stats_dict = model_class.step(model, loss_func, data, train_dataset, device, epoch, mode='train', use_gt_p=sched_samp_gt_p)\n",
    "    #             if torch.isnan(loss).item():\n",
    "    #                 Logger.log('WARNING: NaN loss. Skipping to next data...')\n",
    "    #                 torch.cuda.empty_cache()\n",
    "    #                 continue\n",
    "    #             # backprop and step\n",
    "    #             loss.backward()\n",
    "    #             # check gradients\n",
    "    #             parameters = [p for p in model.parameters() if p.grad is not None]\n",
    "    #             total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), 2.0).to(device) for p in parameters]), 2.0)\n",
    "    #             if torch.isnan(total_norm):\n",
    "    #                 Logger.log('WARNING: NaN gradients. Skipping to next data...')\n",
    "    #                 torch.cuda.empty_cache()\n",
    "    #                 continue\n",
    "    #             optimizer.step()\n",
    "    #         except (RuntimeError, AssertionError) as e:\n",
    "    #             if epoch > 0:\n",
    "    #                 # to catch bad dynamics, but keep training\n",
    "    #                 Logger.log('WARNING: caught an exception during forward or backward pass. Skipping to next data...')\n",
    "    #                 Logger.log(e)\n",
    "    #                 traceback.print_exc()\n",
    "    #                 reset_loss_track = train_dataset.pre_batch(epoch=epoch)\n",
    "    #                 if reset_loss_track:\n",
    "    #                     Logger.log('Resetting min_val_loss and min_train_loss')\n",
    "    #                     min_val_loss = min_train_loss = float('inf')\n",
    "    #                 continue\n",
    "    #             else:\n",
    "    #                 raise e\n",
    "\n",
    "    #         # collect stats\n",
    "    #         batch_elapsed_t = time.time() - batch_start_t\n",
    "    #         total_elapsed_t = time.time() - train_start_t\n",
    "    #         stats_dict['loss'] = loss\n",
    "    #         for param_group in optimizer.param_groups:\n",
    "    #             stats_dict['lr'] = torch.Tensor([param_group['lr']])[0]\n",
    "    #         stats_dict['time_per_batch'] = torch.Tensor([batch_elapsed_t])[0]\n",
    "\n",
    "    #         last_batch = (i==(len(train_loader)-1))\n",
    "    #         stat_tracker.update(stats_dict, tag='train', save_tf=last_batch)\n",
    "    #         if i % args.print_every == 0:\n",
    "    #             stat_tracker.print(i, len(train_loader),\n",
    "    #                             epoch, args.epochs,\n",
    "    #                             total_elapsed_time=total_elapsed_t,\n",
    "    #                             tag='train')\n",
    "\n",
    "    #         reset_loss_track = train_dataset.pre_batch(epoch=epoch)\n",
    "    #         if reset_loss_track:\n",
    "    #             Logger.log('Resetting min_val_loss and min_train_loss')\n",
    "    #             min_val_loss = min_train_loss = float('inf')\n",
    "\n",
    "    #     # save if desired\n",
    "    #     if epoch % args.save_every == 0:\n",
    "    #         Logger.log('Saving checkpoint...')\n",
    "    #         save_file = os.path.join(ckpts_path, 'epoch_%08d_model.pth' % (epoch))\n",
    "    #         save_state(save_file, model, optimizer, cur_epoch=epoch, min_val_loss=min_val_loss, min_train_loss=min_train_loss, ignore_keys=model.ignore_keys)\n",
    "\n",
    "    #     # check if it's the best train model so far\n",
    "    #     mean_train_loss = stat_tracker.meter_dict['train/loss'].avg\n",
    "    #     if mean_train_loss < min_train_loss:\n",
    "    #         min_train_loss = mean_train_loss\n",
    "    #         Logger.log('Best train loss so far! Saving checkpoint...')\n",
    "    #         save_file = os.path.join(ckpts_path, 'best_train_model.pth')\n",
    "    #         save_state(save_file, model, optimizer, cur_epoch=epoch, min_val_loss=min_val_loss, min_train_loss=min_train_loss, ignore_keys=model.ignore_keys)\n",
    "\n",
    "    #     # validate\n",
    "    #     if epoch % args.val_every == 0:\n",
    "    #         with torch.no_grad():\n",
    "    #             # run on validation data\n",
    "    #             model.eval()\n",
    "\n",
    "    #             stat_tracker.reset()\n",
    "    #             for i, data in enumerate(val_loader):\n",
    "    #                 # print(i)\n",
    "    #                 batch_start_t = time.time()\n",
    "    #                 # run model\n",
    "    #                 loss, stats_dict = model_class.step(model, loss_func, data, val_dataset, device, epoch, mode='test', use_gt_p=sched_samp_gt_p)\n",
    "\n",
    "    #                 if torch.isnan(loss):\n",
    "    #                     Logger.log('WARNING: NaN loss on VALIDATION. Skipping to next data...')\n",
    "    #                     continue\n",
    "\n",
    "    #                 # collect stats\n",
    "    #                 batch_elapsed_t = time.time() - batch_start_t\n",
    "    #                 total_elapsed_t = time.time() - train_start_t\n",
    "    #                 stats_dict['loss'] = loss\n",
    "    #                 stats_dict['time_per_batch'] = torch.Tensor([batch_elapsed_t])[0]\n",
    "\n",
    "    #                 stat_tracker.update(stats_dict, tag='val', save_tf=(i==(len(val_loader)-1)), increment_step=False)\n",
    "\n",
    "    #                 if i % args.print_every == 0:\n",
    "    #                     stat_tracker.print(i, len(val_loader),\n",
    "    #                                     epoch, args.epochs,\n",
    "    #                                     total_elapsed_time=total_elapsed_t,\n",
    "    #                                     tag='val')\n",
    "\n",
    "    #             # check if it's the best model so far\n",
    "    #             mean_val_loss = stat_tracker.meter_dict['val/loss'].avg\n",
    "    #             if mean_val_loss < min_val_loss:\n",
    "    #                 min_val_loss = mean_val_loss\n",
    "    #                 Logger.log('Best val loss so far! Saving checkpoint...')\n",
    "    #                 save_file = os.path.join(ckpts_path, 'best_model.pth')\n",
    "    #                 save_state(save_file, model, optimizer, cur_epoch=epoch, min_val_loss=min_val_loss, min_train_loss=min_train_loss, ignore_keys=model.ignore_keys)\n",
    "\n",
    "    #     scheduler.step()\n",
    "\n",
    "    #     torch.cuda.empty_cache()\n",
    "\n",
    "    # Logger.log('Finished!')\n",
    "    print('Finished!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS280",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
