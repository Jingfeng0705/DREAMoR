{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec7b0e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, os\n",
    "\n",
    "import importlib, time\n",
    "import traceback\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from humor.utils.config_new import ConfigParser\n",
    "from humor.utils.logging import Logger, class_name_to_file_name, mkdir, cp_files\n",
    "from humor.utils.torch import get_device, save_state, load_state\n",
    "from humor.utils.stats import StatTracker\n",
    "from humor.models.model_utils import step\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "160908ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default: {'beta2', 'beta1', 'decay', 'ckpt', 'use_adam', 'load_optim', 'eps'}\n",
      "Using default: {'detach_sched_samp', 'output_delta', 'model_use_smpl_joint_inputs'}\n",
      "Using default: {'splits_path', 'data_noise_std', 'frames_out_step_size'}\n",
      "Using default: {'kl_loss_cycle_len', 'smpl_vert_consistency_loss'}\n"
     ]
    }
   ],
   "source": [
    "config_file = \"configs/train_testmodel.yaml\"\n",
    "config_parser_yaml = ConfigParser(config_file)\n",
    "args_obj, _ = config_parser_yaml.parse('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f53ace0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_dict: {'dataset': 'AmassDiscreteDataset', 'model': 'TestModel', 'loss': 'HumorLoss', 'out': './out/testmodel_train_test', 'ckpt': None, 'gpu': 0, 'batch_size': 200, 'print_every': 10, 'epochs': 200, 'val_every': 2, 'save_every': 25, 'lr': 0.0001, 'beta1': 0.9, 'beta2': 0.999, 'eps': 1e-08, 'sched_milestones': [50, 80, 140], 'sched_decay': 0.5, 'decay': 0.0, 'load_optim': True, 'use_adam': False, 'sched_samp_start': 10, 'sched_samp_end': 20}\n",
      "model_dict: {'out_rot_rep': 'aa', 'in_rot_rep': 'mat', 'latent_size': 48, 'steps_in': 1, 'conditional_prior': False, 'output_delta': True, 'model_data_config': 'smpl+joints+contacts', 'detach_sched_samp': True, 'model_use_smpl_joint_inputs': False}\n",
      "dataset_dict: {'data_paths': ['../datasets/AMASS/amass_processed'], 'split_by': 'sequence', 'splits_path': None, 'sample_num_frames': 10, 'data_rot_rep': 'mat', 'step_frames_in': 1, 'step_frames_out': 1, 'frames_out_step_size': 1, 'data_return_config': 'smpl+joints+contacts', 'data_noise_std': 0.0}\n",
      "loss_dict: {'kl_loss': 0.0004, 'kl_loss_anneal_start': 0, 'kl_loss_anneal_end': 50, 'kl_loss_cycle_len': -1, 'regr_trans_loss': 1.0, 'regr_trans_vel_loss': 1.0, 'regr_root_orient_loss': 1.0, 'regr_root_orient_vel_loss': 1.0, 'regr_pose_loss': 1.0, 'regr_pose_vel_loss': 1.0, 'regr_joint_loss': 1.0, 'regr_joint_vel_loss': 1.0, 'regr_joint_orient_vel_loss': 1.0, 'regr_vert_loss': 1.0, 'regr_vert_vel_loss': 1.0, 'contacts_loss': 0.01, 'contacts_vel_loss': 0.01, 'smpl_joint_loss': 1.0, 'smpl_mesh_loss': 1.0, 'smpl_joint_consistency_loss': 1.0, 'smpl_vert_consistency_loss': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# See config\n",
    "dict_attr = ['base_dict', 'model_dict', 'dataset_dict', 'loss_dict']\n",
    "for attr in dict_attr:\n",
    "    print(f\"{attr}: {getattr(args_obj, attr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cba45594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base args: namespace(dataset='AmassDiscreteDataset', model='TestModel', loss='HumorLoss', out='./out/testmodel_train_test\\\\20250424_151240', ckpt=None, gpu=0, batch_size=200, print_every=10, epochs=200, val_every=2, save_every=25, lr=0.0001, beta1=0.9, beta2=0.999, eps=1e-08, sched_milestones=[50, 80, 140], sched_decay=0.5, decay=0.0, load_optim=True, use_adam=False, sched_samp_start=10, sched_samp_end=20)\n",
      "Model args: namespace(out_rot_rep='aa', in_rot_rep='mat', latent_size=48, steps_in=1, conditional_prior=False, output_delta=True, model_data_config='smpl+joints+contacts', detach_sched_samp=True, model_use_smpl_joint_inputs=False)\n",
      "Dataset args: namespace(data_paths=['../datasets/AMASS/amass_processed'], split_by='sequence', splits_path=None, sample_num_frames=10, data_rot_rep='mat', step_frames_in=1, step_frames_out=1, frames_out_step_size=1, data_return_config='smpl+joints+contacts', data_noise_std=0.0)\n",
      "Loss args: namespace(kl_loss=0.0004, kl_loss_anneal_start=0, kl_loss_anneal_end=50, kl_loss_cycle_len=-1, regr_trans_loss=1.0, regr_trans_vel_loss=1.0, regr_root_orient_loss=1.0, regr_root_orient_vel_loss=1.0, regr_pose_loss=1.0, regr_pose_vel_loss=1.0, regr_joint_loss=1.0, regr_joint_vel_loss=1.0, regr_joint_orient_vel_loss=1.0, regr_vert_loss=1.0, regr_vert_vel_loss=1.0, contacts_loss=0.01, contacts_vel_loss=0.01, smpl_joint_loss=1.0, smpl_mesh_loss=1.0, smpl_joint_consistency_loss=1.0, smpl_vert_consistency_loss=0.0)\n"
     ]
    }
   ],
   "source": [
    "args = args_obj.base\n",
    "args.out = os.path.join(args.out, time.strftime('%Y%m%d_%H%M%S'))\n",
    "mkdir(args.out)\n",
    "train_log_path = os.path.join(args.out, 'train.log')\n",
    "Logger.init(train_log_path)\n",
    "   \n",
    "# save arguments used\n",
    "Logger.log('Base args: ' + str(args))\n",
    "Logger.log('Model args: ' + str(args_obj.model))\n",
    "Logger.log('Dataset args: ' + str(args_obj.dataset))\n",
    "Logger.log('Loss args: ' + str(args_obj.loss))\n",
    "\n",
    "train_scripts_path = os.path.join(args.out, 'train_scripts')\n",
    "mkdir(train_scripts_path)\n",
    "pkg_root = \"humor\"\n",
    "dataset_file = class_name_to_file_name(args.dataset)\n",
    "dataset_file_path = os.path.join(pkg_root, 'datasets/' + dataset_file + '.py')\n",
    "model_file = class_name_to_file_name(args.model)\n",
    "model_file_path = os.path.join(pkg_root, 'models/' + model_file + '.py')\n",
    "loss_file = class_name_to_file_name(args.loss)\n",
    "train_file_path = \"train.ipynb\"\n",
    "cp_files(train_scripts_path, [train_file_path, model_file_path, dataset_file_path, config_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13e55124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model class:  <module 'models.test_model' from 'E:\\\\workspace\\\\Motion\\\\humor\\\\humor\\\\utils\\\\..\\\\models\\\\test_model.py'>\n"
     ]
    }
   ],
   "source": [
    "model_class = importlib.import_module('models.' + model_file)\n",
    "print('Model class: ', model_class)\n",
    "Model = getattr(model_class, args.model)\n",
    "model = Model(**args_obj.model_dict,\n",
    "                model_smpl_batch_size=args.batch_size) # assumes model is HumorModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1313e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss class:  <module 'losses.humor_loss' from 'E:\\\\workspace\\\\Motion\\\\humor\\\\humor\\\\utils\\\\..\\\\losses\\\\humor_loss.py'>\n"
     ]
    }
   ],
   "source": [
    "  # load loss class and instantiate\n",
    "loss_class = importlib.import_module('losses.' + loss_file)\n",
    "print('Loss class: ', loss_class)\n",
    "Loss = getattr(loss_class, args.loss)\n",
    "loss_func = Loss(**args_obj.loss_dict,\n",
    "                    smpl_batch_size=args.batch_size*args_obj.dataset.sample_num_frames) # assumes loss is HumorLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "641c2f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using detected GPU...\n",
      "TestModel(\n",
      "  (mlp_test): Sequential(\n",
      "    (0): Linear(in_features=339, out_features=512, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (4): ELU(alpha=1.0)\n",
      "    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (7): ELU(alpha=1.0)\n",
      "    (8): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): Linear(in_features=512, out_features=96, bias=True)\n",
      "  )\n",
      "  (decode_test): Sequential(\n",
      "    (0): Linear(in_features=48, out_features=512, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (4): ELU(alpha=1.0)\n",
      "    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (7): ELU(alpha=1.0)\n",
      "    (8): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): Linear(in_features=512, out_features=216, bias=True)\n",
      "  )\n",
      ")\n",
      "Num model params: 2467640\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = get_device(args.gpu)\n",
    "model.to(device)\n",
    "loss_func.to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# count params in model\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "Logger.log('Num model params: ' + str(params))\n",
    "\n",
    "# freeze params in loss\n",
    "for param in loss_func.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93b1fca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using scheduled sampling starting at epoch 10 and ending at epoch 20!\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "betas = (args.beta1, args.beta2)\n",
    "if args.use_adam:\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                            lr=args.lr,\n",
    "                            betas=betas,\n",
    "                            eps=args.eps,\n",
    "                            weight_decay=args.decay)\n",
    "else:\n",
    "    optimizer = optim.Adamax(model.parameters(),\n",
    "                            lr=args.lr,\n",
    "                            betas=betas,\n",
    "                            eps=args.eps,\n",
    "                            weight_decay=args.decay)\n",
    "\n",
    "# load in pretrained weights/optimizer state if given\n",
    "start_epoch = 0\n",
    "min_val_loss = min_train_loss = float('inf')\n",
    "if args.ckpt is not None:\n",
    "    load_optim = optimizer if args.load_optim else None\n",
    "    start_epoch, min_val_loss, min_train_loss = load_state(args.ckpt, model, optimizer=load_optim, map_location=device, ignore_keys=model.ignore_keys)\n",
    "    start_epoch += 1\n",
    "    Logger.log('Resuming from saved checkpoint at epoch idx %d with min val loss %.6f...' % (start_epoch, min_val_loss))\n",
    "    if not args.load_optim:\n",
    "        Logger.log('Not loading optimizer state as desired...')\n",
    "        Logger.log('WARNING: Also resetting min_val_loss and epoch count!')\n",
    "        min_val_loss = float('inf')\n",
    "        start_epoch = 0\n",
    "\n",
    "# initialize LR scheduler\n",
    "scheduler = MultiStepLR(optimizer, milestones=args.sched_milestones, gamma=args.sched_decay)\n",
    "\n",
    "# intialize schedule sampling if desired\n",
    "use_sched_samp = False\n",
    "if args.sched_samp_start is not None and args.sched_samp_end is not None:\n",
    "    if args.sched_samp_start >= 0 and args.sched_samp_end >= args.sched_samp_start:\n",
    "        Logger.log('Using scheduled sampling starting at epoch %d and ending at epoch %d!' % (args.sched_samp_start, args.sched_samp_end))\n",
    "        use_sched_samp = True\n",
    "    else:\n",
    "        Logger.log('Could not use scheduled sampling with given start and end!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "035ae111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class:  <class 'datasets.amass_discrete_dataset.AmassDiscreteDataset'>\n",
      "Loading data from../datasets/AMASS/amass_processed\n",
      "Logger must be initialized before logging!\n",
      "This split contains 286 sequences (that meet the duration criteria).\n",
      "Logger must be initialized before logging!\n",
      "The dataset contains 18411 sub-sequences in total.\n",
      "Logger must be initialized before logging!\n",
      "Loading data from../datasets/AMASS/amass_processed\n",
      "Logger must be initialized before logging!\n",
      "This split contains 24 sequences (that meet the duration criteria).\n",
      "Logger must be initialized before logging!\n",
      "The dataset contains 739 sub-sequences in total.\n",
      "Logger must be initialized before logging!\n"
     ]
    }
   ],
   "source": [
    "# load dataset class and instantiate training and validation set\n",
    "Dataset = getattr(importlib.import_module('datasets.' + dataset_file), args.dataset)\n",
    "print('Dataset class: ', Dataset)\n",
    "train_dataset = Dataset(split='train', **args_obj.dataset_dict)\n",
    "val_dataset = Dataset(split='val', **args_obj.dataset_dict)\n",
    "# create loaders\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            pin_memory=True,\n",
    "                            worker_init_fn=lambda _: np.random.seed()) # get around pytorch RNG seed bug\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=args.batch_size,\n",
    "                        shuffle=False, \n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=True,\n",
    "                        worker_init_fn=lambda _: np.random.seed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93bd7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats tracker\n",
    "tensorboard_path = os.path.join(args.out, 'train_tensorboard')\n",
    "mkdir(tensorboard_path)\n",
    "stat_tracker = StatTracker(tensorboard_path, train_log_path)\n",
    "\n",
    "# checkpoints saving\n",
    "ckpts_path = os.path.join(args.out, 'checkpoints')\n",
    "mkdir(ckpts_path)\n",
    "\n",
    "if use_sched_samp:\n",
    "    train_dataset.return_global = True\n",
    "    val_dataset.return_global = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ce692d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled sampling current use_gt_p = 1.000000\n",
      "[>--------------------------------------------------] train epoch - 1/200 | batch - 1/93\n",
      "1.084 s per batch | 1 s elapsed | 19 d 17 h 58 m 27 s ETA\n",
      "train/kl_loss : 12.15926\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.30654\n",
      "train/trans_vel_loss : 0.33588\n",
      "train/root_orient_loss : 0.20713\n",
      "train/root_orient_vel_loss : 0.68824\n",
      "train/pose_body_loss : 0.19505\n",
      "train/joints_loss : 0.33051\n",
      "train/joints_vel_loss : 0.44982\n",
      "train/contacts_loss : 0.74201\n",
      "train/contacts_acc : 0.48678\n",
      "train/contacts_pos_acc : 0.49899\n",
      "train/contacts_neg_acc : 0.48148\n",
      "train/contacts_vel_loss : 2.30054\n",
      "train/smpl_joint_loss : 0.39611\n",
      "train/smpl_mesh_loss : 0.42806\n",
      "train/smpl_joint_consistency_loss : 0.74142\n",
      "train/reconstr_weighted_loss : 4.10920\n",
      "train/loss : 4.10920\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 1.08368\n",
      "[=====>---------------------------------------------] train epoch - 1/200 | batch - 11/93\n",
      "0.485 s per batch | 11 s elapsed | 6 h 7 m 4 s ETA\n",
      "train/kl_loss : 15.64204\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.07743\n",
      "train/trans_vel_loss : 0.12906\n",
      "train/root_orient_loss : 0.09405\n",
      "train/root_orient_vel_loss : 0.54217\n",
      "train/pose_body_loss : 0.17488\n",
      "train/joints_loss : 0.26017\n",
      "train/joints_vel_loss : 0.38750\n",
      "train/contacts_loss : 0.73130\n",
      "train/contacts_acc : 0.50715\n",
      "train/contacts_pos_acc : 0.53170\n",
      "train/contacts_neg_acc : 0.49696\n",
      "train/contacts_vel_loss : 2.05565\n",
      "train/smpl_joint_loss : 0.12872\n",
      "train/smpl_mesh_loss : 0.15177\n",
      "train/smpl_joint_consistency_loss : 0.38329\n",
      "train/reconstr_weighted_loss : 2.35692\n",
      "train/loss : 2.35692\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.48451\n",
      "[===========>---------------------------------------] train epoch - 1/200 | batch - 21/93\n",
      "0.457 s per batch | 21 s elapsed | 5 h 40 m 16 s ETA\n",
      "train/kl_loss : 18.79574\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.05418\n",
      "train/trans_vel_loss : 0.09163\n",
      "train/root_orient_loss : 0.06299\n",
      "train/root_orient_vel_loss : 0.51119\n",
      "train/pose_body_loss : 0.15314\n",
      "train/joints_loss : 0.20604\n",
      "train/joints_vel_loss : 0.34262\n",
      "train/contacts_loss : 0.72615\n",
      "train/contacts_acc : 0.51442\n",
      "train/contacts_pos_acc : 0.54803\n",
      "train/contacts_neg_acc : 0.50053\n",
      "train/contacts_vel_loss : 1.92037\n",
      "train/smpl_joint_loss : 0.09381\n",
      "train/smpl_mesh_loss : 0.11292\n",
      "train/smpl_joint_consistency_loss : 0.29373\n",
      "train/reconstr_weighted_loss : 1.94872\n",
      "train/loss : 1.94872\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.45672\n",
      "[================>----------------------------------] train epoch - 1/200 | batch - 31/93\n",
      "0.447 s per batch | 32 s elapsed | 5 h 30 m 48 s ETA\n",
      "train/kl_loss : 21.62339\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.04343\n",
      "train/trans_vel_loss : 0.07578\n",
      "train/root_orient_loss : 0.04965\n",
      "train/root_orient_vel_loss : 0.47891\n",
      "train/pose_body_loss : 0.13374\n",
      "train/joints_loss : 0.16951\n",
      "train/joints_vel_loss : 0.29705\n",
      "train/contacts_loss : 0.72178\n",
      "train/contacts_acc : 0.51954\n",
      "train/contacts_pos_acc : 0.56420\n",
      "train/contacts_neg_acc : 0.50118\n",
      "train/contacts_vel_loss : 1.75440\n",
      "train/smpl_joint_loss : 0.07605\n",
      "train/smpl_mesh_loss : 0.09211\n",
      "train/smpl_joint_consistency_loss : 0.23967\n",
      "train/reconstr_weighted_loss : 1.68066\n",
      "train/loss : 1.68066\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.44722\n",
      "[======================>----------------------------] train epoch - 1/200 | batch - 41/93\n",
      "0.443 s per batch | 42 s elapsed | 5 h 26 m 48 s ETA\n",
      "train/kl_loss : 24.04883\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.03678\n",
      "train/trans_vel_loss : 0.06624\n",
      "train/root_orient_loss : 0.04168\n",
      "train/root_orient_vel_loss : 0.46883\n",
      "train/pose_body_loss : 0.11819\n",
      "train/joints_loss : 0.14419\n",
      "train/joints_vel_loss : 0.26801\n",
      "train/contacts_loss : 0.71809\n",
      "train/contacts_acc : 0.52389\n",
      "train/contacts_pos_acc : 0.58114\n",
      "train/contacts_neg_acc : 0.50053\n",
      "train/contacts_vel_loss : 1.64693\n",
      "train/smpl_joint_loss : 0.06463\n",
      "train/smpl_mesh_loss : 0.07855\n",
      "train/smpl_joint_consistency_loss : 0.20325\n",
      "train/reconstr_weighted_loss : 1.51400\n",
      "train/loss : 1.51400\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.44295\n",
      "[===========================>-----------------------] train epoch - 1/200 | batch - 51/93\n",
      "0.440 s per batch | 52 s elapsed | 5 h 23 m 18 s ETA\n",
      "train/kl_loss : 26.15292\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.03219\n",
      "train/trans_vel_loss : 0.05980\n",
      "train/root_orient_loss : 0.03632\n",
      "train/root_orient_vel_loss : 0.46666\n",
      "train/pose_body_loss : 0.10582\n",
      "train/joints_loss : 0.12599\n",
      "train/joints_vel_loss : 0.24922\n",
      "train/contacts_loss : 0.71506\n",
      "train/contacts_acc : 0.52737\n",
      "train/contacts_pos_acc : 0.59678\n",
      "train/contacts_neg_acc : 0.49882\n",
      "train/contacts_vel_loss : 1.55657\n",
      "train/smpl_joint_loss : 0.05670\n",
      "train/smpl_mesh_loss : 0.06904\n",
      "train/smpl_joint_consistency_loss : 0.17749\n",
      "train/reconstr_weighted_loss : 1.40194\n",
      "train/loss : 1.40194\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.43977\n",
      "[================================>------------------] train epoch - 1/200 | batch - 61/93\n",
      "0.437 s per batch | 1 m 2 s elapsed | 5 h 21 m 40 s ETA\n",
      "train/kl_loss : 27.96120\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.02878\n",
      "train/trans_vel_loss : 0.05498\n",
      "train/root_orient_loss : 0.03242\n",
      "train/root_orient_vel_loss : 0.46318\n",
      "train/pose_body_loss : 0.09591\n",
      "train/joints_loss : 0.11228\n",
      "train/joints_vel_loss : 0.23621\n",
      "train/contacts_loss : 0.71351\n",
      "train/contacts_acc : 0.52922\n",
      "train/contacts_pos_acc : 0.61086\n",
      "train/contacts_neg_acc : 0.49550\n",
      "train/contacts_vel_loss : 1.48156\n",
      "train/smpl_joint_loss : 0.05076\n",
      "train/smpl_mesh_loss : 0.06191\n",
      "train/smpl_joint_consistency_loss : 0.15818\n",
      "train/reconstr_weighted_loss : 1.31656\n",
      "train/loss : 1.31656\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.43702\n",
      "[======================================>------------] train epoch - 1/200 | batch - 71/93\n",
      "0.435 s per batch | 1 m 12 s elapsed | 5 h 20 m 6 s ETA\n",
      "train/kl_loss : 29.72570\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.02614\n",
      "train/trans_vel_loss : 0.05143\n",
      "train/root_orient_loss : 0.02940\n",
      "train/root_orient_vel_loss : 0.45666\n",
      "train/pose_body_loss : 0.08782\n",
      "train/joints_loss : 0.10156\n",
      "train/joints_vel_loss : 0.22131\n",
      "train/contacts_loss : 0.71247\n",
      "train/contacts_acc : 0.53072\n",
      "train/contacts_pos_acc : 0.62431\n",
      "train/contacts_neg_acc : 0.49206\n",
      "train/contacts_vel_loss : 1.41173\n",
      "train/smpl_joint_loss : 0.04612\n",
      "train/smpl_mesh_loss : 0.05630\n",
      "train/smpl_joint_consistency_loss : 0.14312\n",
      "train/reconstr_weighted_loss : 1.24111\n",
      "train/loss : 1.24111\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.43461\n",
      "[===========================================>-------] train epoch - 1/200 | batch - 81/93\n",
      "0.433 s per batch | 1 m 22 s elapsed | 5 h 17 m 51 s ETA\n",
      "train/kl_loss : 31.32625\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.02400\n",
      "train/trans_vel_loss : 0.04843\n",
      "train/root_orient_loss : 0.02700\n",
      "train/root_orient_vel_loss : 0.45208\n",
      "train/pose_body_loss : 0.08109\n",
      "train/joints_loss : 0.09295\n",
      "train/joints_vel_loss : 0.20964\n",
      "train/contacts_loss : 0.71221\n",
      "train/contacts_acc : 0.53160\n",
      "train/contacts_pos_acc : 0.63652\n",
      "train/contacts_neg_acc : 0.48830\n",
      "train/contacts_vel_loss : 1.35140\n",
      "train/smpl_joint_loss : 0.04238\n",
      "train/smpl_mesh_loss : 0.05177\n",
      "train/smpl_joint_consistency_loss : 0.13102\n",
      "train/reconstr_weighted_loss : 1.18099\n",
      "train/loss : 1.18099\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.43288\n",
      "[================================================>--] train epoch - 1/200 | batch - 91/93\n",
      "0.431 s per batch | 1 m 32 s elapsed | 5 h 16 m 28 s ETA\n",
      "train/kl_loss : 32.84622\n",
      "train/kl_anneal_weight : 0.00000\n",
      "train/kl_weighted_loss : 0.00000\n",
      "train/trans_loss : 0.02222\n",
      "train/trans_vel_loss : 0.04586\n",
      "train/root_orient_loss : 0.02502\n",
      "train/root_orient_vel_loss : 0.44192\n",
      "train/pose_body_loss : 0.07545\n",
      "train/joints_loss : 0.08588\n",
      "train/joints_vel_loss : 0.19895\n",
      "train/contacts_loss : 0.71276\n",
      "train/contacts_acc : 0.53154\n",
      "train/contacts_pos_acc : 0.64707\n",
      "train/contacts_neg_acc : 0.48402\n",
      "train/contacts_vel_loss : 1.30109\n",
      "train/smpl_joint_loss : 0.03929\n",
      "train/smpl_mesh_loss : 0.04801\n",
      "train/smpl_joint_consistency_loss : 0.12109\n",
      "train/reconstr_weighted_loss : 1.12382\n",
      "train/loss : 1.12382\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.43149\n",
      "Saving checkpoint...\n",
      "Best train loss so far! Saving checkpoint...\n",
      "[============>--------------------------------------] val epoch - 1/200 | batch - 1/4\n",
      "0.198 s per batch | 1 m 34 s elapsed | 1093 d 16 h 25 m 26 s ETA\n",
      "val/kl_loss : 49.70377\n",
      "val/kl_anneal_weight : 0.00000\n",
      "val/kl_weighted_loss : 0.00000\n",
      "val/trans_loss : 0.00828\n",
      "val/trans_vel_loss : 0.02589\n",
      "val/root_orient_loss : 0.00961\n",
      "val/root_orient_vel_loss : 0.28470\n",
      "val/pose_body_loss : 0.03015\n",
      "val/joints_loss : 0.02981\n",
      "val/joints_vel_loss : 0.07696\n",
      "val/contacts_loss : 0.82471\n",
      "val/contacts_acc : 0.48989\n",
      "val/contacts_pos_acc : 0.80900\n",
      "val/contacts_neg_acc : 0.35863\n",
      "val/contacts_vel_loss : 1.06095\n",
      "val/smpl_joint_loss : 0.01472\n",
      "val/smpl_mesh_loss : 0.01815\n",
      "val/smpl_joint_consistency_loss : 0.04272\n",
      "val/reconstr_weighted_loss : 0.55984\n",
      "val/loss : 0.55984\n",
      "val/time_per_batch : 0.19805\n",
      "Best val loss so far! Saving checkpoint...\n",
      "Scheduled sampling current use_gt_p = 1.000000\n",
      "[>--------------------------------------------------] train epoch - 2/200 | batch - 1/93\n",
      "0.503 s per batch | 1 m 38 s elapsed | 5 h 25 m 9 s ETA\n",
      "train/kl_loss : 49.10458\n",
      "train/kl_anneal_weight : 0.02000\n",
      "train/kl_weighted_loss : 0.00039\n",
      "train/trans_loss : 0.00711\n",
      "train/trans_vel_loss : 0.02573\n",
      "train/root_orient_loss : 0.00847\n",
      "train/root_orient_vel_loss : 0.46839\n",
      "train/pose_body_loss : 0.02636\n",
      "train/joints_loss : 0.02569\n",
      "train/joints_vel_loss : 0.09406\n",
      "train/contacts_loss : 0.70489\n",
      "train/contacts_acc : 0.54600\n",
      "train/contacts_pos_acc : 0.74783\n",
      "train/contacts_neg_acc : 0.46690\n",
      "train/contacts_vel_loss : 0.95794\n",
      "train/smpl_joint_loss : 0.01281\n",
      "train/smpl_mesh_loss : 0.01573\n",
      "train/smpl_joint_consistency_loss : 0.03657\n",
      "train/reconstr_weighted_loss : 0.73757\n",
      "train/loss : 0.73796\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.50253\n",
      "[=====>---------------------------------------------] train epoch - 2/200 | batch - 11/93\n",
      "0.427 s per batch | 1 m 48 s elapsed | 5 h 23 m 18 s ETA\n",
      "train/kl_loss : 47.81597\n",
      "train/kl_anneal_weight : 0.02000\n",
      "train/kl_weighted_loss : 0.00038\n",
      "train/trans_loss : 0.00719\n",
      "train/trans_vel_loss : 0.02604\n",
      "train/root_orient_loss : 0.00825\n",
      "train/root_orient_vel_loss : 0.43994\n",
      "train/pose_body_loss : 0.02616\n",
      "train/joints_loss : 0.02540\n",
      "train/joints_vel_loss : 0.12098\n",
      "train/contacts_loss : 0.72015\n",
      "train/contacts_acc : 0.53758\n",
      "train/contacts_pos_acc : 0.73981\n",
      "train/contacts_neg_acc : 0.45648\n",
      "train/contacts_vel_loss : 0.92558\n",
      "train/smpl_joint_loss : 0.01289\n",
      "train/smpl_mesh_loss : 0.01594\n",
      "train/smpl_joint_consistency_loss : 0.03634\n",
      "train/reconstr_weighted_loss : 0.73557\n",
      "train/loss : 0.73596\n",
      "train/lr : 0.00010\n",
      "train/time_per_batch : 0.42688\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m     Logger.log(\u001b[33m'\u001b[39m\u001b[33mResetting min_val_loss and min_train_loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     34\u001b[39m     min_val_loss = min_train_loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_start_t\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# zero the gradients\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\newProgramFiles\\tool\\anaconda\\envs\\CS280\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    628\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    633\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\newProgramFiles\\tool\\anaconda\\envs\\CS280\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    672\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    674\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    675\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\newProgramFiles\\tool\\anaconda\\envs\\CS280\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\newProgramFiles\\tool\\anaconda\\envs\\CS280\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\workspace\\Motion\\humor\\humor\\utils\\..\\datasets\\amass_discrete_dataset.py:257\u001b[39m, in \u001b[36m__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    254\u001b[39m pose_body_vel = data[\u001b[33m'\u001b[39m\u001b[33mpose_body_vel\u001b[39m\u001b[33m'\u001b[39m][sample_frame_inds]\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# Joints\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m joints = data[\u001b[33m'\u001b[39m\u001b[33mjoints\u001b[39m\u001b[33m'\u001b[39m][sample_frame_inds]\n\u001b[32m    258\u001b[39m verts = data[\u001b[33m'\u001b[39m\u001b[33mmojo_verts\u001b[39m\u001b[33m'\u001b[39m][sample_frame_inds]\n\u001b[32m    259\u001b[39m num_verts = verts.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\newProgramFiles\\tool\\anaconda\\envs\\CS280\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:256\u001b[39m, in \u001b[36mNpzFile.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m magic == \u001b[38;5;28mformat\u001b[39m.MAGIC_PREFIX:\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mbytes\u001b[39m = \u001b[38;5;28mself\u001b[39m.zip.open(key)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.zip.read(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\newProgramFiles\\tool\\anaconda\\envs\\CS280\\Lib\\site-packages\\numpy\\lib\\format.py:857\u001b[39m, in \u001b[36mread_array\u001b[39m\u001b[34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[39m\n\u001b[32m    855\u001b[39m             read_size = \u001b[38;5;28mint\u001b[39m(read_count * dtype.itemsize)\n\u001b[32m    856\u001b[39m             data = _read_bytes(fp, read_size, \u001b[33m\"\u001b[39m\u001b[33marray data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m857\u001b[39m             array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[32m    858\u001b[39m                                                      count=read_count)\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fortran_order:\n\u001b[32m    861\u001b[39m     array.shape = shape[::-\u001b[32m1\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# main training loop\n",
    "train_start_t = time.time()\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # train\n",
    "    stat_tracker.reset()\n",
    "    batch_start_t = None\n",
    "    reset_loss_track = train_dataset.pre_batch(epoch=epoch)\n",
    "    # see which phase we're in \n",
    "    sched_samp_gt_p = 1.0 # supervised\n",
    "    if use_sched_samp:\n",
    "        if epoch >= args.sched_samp_start and epoch < args.sched_samp_end:\n",
    "            frac = (epoch - args.sched_samp_start) / (args.sched_samp_end - args.sched_samp_start)\n",
    "            sched_samp_gt_p = 1.0*(1.0 - frac)\n",
    "        elif epoch >= args.sched_samp_end:\n",
    "            # autoregressive\n",
    "            sched_samp_gt_p = 0.0\n",
    "        Logger.log('Scheduled sampling current use_gt_p = %f' % (sched_samp_gt_p))\n",
    "\n",
    "        if epoch == args.sched_samp_end:\n",
    "            # the loss will naturally go up when using own rollouts\n",
    "            reset_loss_track = True\n",
    "\n",
    "        if args_obj.loss_dict['kl_loss_cycle_len'] > 0:\n",
    "            # if we're cycling, only want to save results when using full ELBO\n",
    "            if (epoch % args_obj.loss_dict['kl_loss_cycle_len']) > (args_obj.loss_dict['kl_loss_cycle_len'] // 2):\n",
    "                # have reached second half of a cycle\n",
    "                reset_loss_track = True\n",
    "\n",
    "    if reset_loss_track:\n",
    "        Logger.log('Resetting min_val_loss and min_train_loss')\n",
    "        min_val_loss = min_train_loss = float('inf')\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        batch_start_t = time.time()\n",
    "\n",
    "        try:\n",
    "            # zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # run model\n",
    "            loss, stats_dict = step(model, loss_func, data, train_dataset, device, epoch, mode='train', use_gt_p=sched_samp_gt_p)\n",
    "            if torch.isnan(loss).item():\n",
    "                Logger.log('WARNING: NaN loss. Skipping to next data...')\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            # backprop and step\n",
    "            loss.backward()\n",
    "            # check gradients\n",
    "            parameters = [p for p in model.parameters() if p.grad is not None]\n",
    "            total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), 2.0).to(device) for p in parameters]), 2.0)\n",
    "            if torch.isnan(total_norm):\n",
    "                Logger.log('WARNING: NaN gradients. Skipping to next data...')\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            optimizer.step()\n",
    "        except (RuntimeError, AssertionError) as e:\n",
    "            if epoch > 0:\n",
    "                # to catch bad dynamics, but keep training\n",
    "                Logger.log('WARNING: caught an exception during forward or backward pass. Skipping to next data...')\n",
    "                Logger.log(e)\n",
    "                traceback.print_exc()\n",
    "                reset_loss_track = train_dataset.pre_batch(epoch=epoch)\n",
    "                if reset_loss_track:\n",
    "                    Logger.log('Resetting min_val_loss and min_train_loss')\n",
    "                    min_val_loss = min_train_loss = float('inf')\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "        # collect stats\n",
    "        batch_elapsed_t = time.time() - batch_start_t\n",
    "        total_elapsed_t = time.time() - train_start_t\n",
    "        stats_dict['loss'] = loss\n",
    "        for param_group in optimizer.param_groups:\n",
    "            stats_dict['lr'] = torch.Tensor([param_group['lr']])[0]\n",
    "        stats_dict['time_per_batch'] = torch.Tensor([batch_elapsed_t])[0]\n",
    "\n",
    "        last_batch = (i==(len(train_loader)-1))\n",
    "        stat_tracker.update(stats_dict, tag='train', save_tf=last_batch)\n",
    "        if i % args.print_every == 0:\n",
    "            stat_tracker.print(i, len(train_loader),\n",
    "                            epoch, args.epochs,\n",
    "                            total_elapsed_time=total_elapsed_t,\n",
    "                            tag='train')\n",
    "\n",
    "        reset_loss_track = train_dataset.pre_batch(epoch=epoch)\n",
    "        if reset_loss_track:\n",
    "            Logger.log('Resetting min_val_loss and min_train_loss')\n",
    "            min_val_loss = min_train_loss = float('inf')\n",
    "\n",
    "    # save if desired\n",
    "    if epoch % args.save_every == 0:\n",
    "        Logger.log('Saving checkpoint...')\n",
    "        save_file = os.path.join(ckpts_path, 'epoch_%08d_model.pth' % (epoch))\n",
    "        save_state(save_file, model, optimizer, cur_epoch=epoch, min_val_loss=min_val_loss, min_train_loss=min_train_loss, ignore_keys=model.ignore_keys)\n",
    "\n",
    "    # check if it's the best train model so far\n",
    "    mean_train_loss = stat_tracker.meter_dict['train/loss'].avg\n",
    "    if mean_train_loss < min_train_loss:\n",
    "        min_train_loss = mean_train_loss\n",
    "        Logger.log('Best train loss so far! Saving checkpoint...')\n",
    "        save_file = os.path.join(ckpts_path, 'best_train_model.pth')\n",
    "        save_state(save_file, model, optimizer, cur_epoch=epoch, min_val_loss=min_val_loss, min_train_loss=min_train_loss, ignore_keys=model.ignore_keys)\n",
    "\n",
    "    # validate\n",
    "    if epoch % args.val_every == 0:\n",
    "        with torch.no_grad():\n",
    "            # run on validation data\n",
    "            model.eval()\n",
    "\n",
    "            stat_tracker.reset()\n",
    "            for i, data in enumerate(val_loader):\n",
    "                # print(i)\n",
    "                batch_start_t = time.time()\n",
    "                # run model\n",
    "                loss, stats_dict = step(model, loss_func, data, val_dataset, device, epoch, mode='test', use_gt_p=sched_samp_gt_p)\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    Logger.log('WARNING: NaN loss on VALIDATION. Skipping to next data...')\n",
    "                    continue\n",
    "\n",
    "                # collect stats\n",
    "                batch_elapsed_t = time.time() - batch_start_t\n",
    "                total_elapsed_t = time.time() - train_start_t\n",
    "                stats_dict['loss'] = loss\n",
    "                stats_dict['time_per_batch'] = torch.Tensor([batch_elapsed_t])[0]\n",
    "\n",
    "                stat_tracker.update(stats_dict, tag='val', save_tf=(i==(len(val_loader)-1)), increment_step=False)\n",
    "\n",
    "                if i % args.print_every == 0:\n",
    "                    stat_tracker.print(i, len(val_loader),\n",
    "                                    epoch, args.epochs,\n",
    "                                    total_elapsed_time=total_elapsed_t,\n",
    "                                    tag='val')\n",
    "\n",
    "            # check if it's the best model so far\n",
    "            mean_val_loss = stat_tracker.meter_dict['val/loss'].avg\n",
    "            if mean_val_loss < min_val_loss:\n",
    "                min_val_loss = mean_val_loss\n",
    "                Logger.log('Best val loss so far! Saving checkpoint...')\n",
    "                save_file = os.path.join(ckpts_path, 'best_model.pth')\n",
    "                save_state(save_file, model, optimizer, cur_epoch=epoch, min_val_loss=min_val_loss, min_train_loss=min_train_loss, ignore_keys=model.ignore_keys)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "Logger.log('Finished!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS280",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
