{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d7632a",
   "metadata": {},
   "source": [
    "If using Colab, run these 3 cells first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ea6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smplx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# from google.colab import userdata\n",
    "# token = userdata.get(\"github_clone\")\n",
    "\n",
    "# repo_url = f\"https://x-access-token:{token}@github.com/Jingfeng0705/humor.git\"\n",
    "# !git clone {repo_url}\n",
    "# %cd /content/humor\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d91081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# import zipfile\n",
    "\n",
    "# # 1. Zip the dataset in Google Drive (do this manually or using Google Colab's file explorer)\n",
    "# #    The zip file should be named 'datasets.zip' and located in '/content/drive/MyDrive/DiffuMoR/'\n",
    "\n",
    "# # 2. Define paths\n",
    "# dataset_zip_gdrive = \"/content/drive/MyDrive/DiffuMoR/datasets/AMASS.zip\"\n",
    "# dataset_zip_local = \"/content/datasets/AMASS.zip\"\n",
    "# dataset_path_local = \"/content/datasets\"\n",
    "\n",
    "# os.makedirs(dataset_path_local, exist_ok=True)\n",
    "\n",
    "# # 3. Copy the zip file\n",
    "# shutil.copy(dataset_zip_gdrive, dataset_zip_local)\n",
    "# print(f\"Dataset zip copied from {dataset_zip_gdrive} to {dataset_zip_local}\")\n",
    "\n",
    "# # 4. Extract the zip file\n",
    "# with zipfile.ZipFile(dataset_zip_local, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(dataset_path_local)\n",
    "# print(f\"Dataset extracted to {dataset_path_local}\")\n",
    "\n",
    "# # 5. (Optional) Remove the zip file after extraction\n",
    "# os.remove(dataset_zip_local) \n",
    "# print(f\"Removed temporary zip file: {dataset_zip_local}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec7b0e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, os\n",
    "\n",
    "import importlib, time\n",
    "import traceback\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dreamor.utils.config_new import ConfigParser\n",
    "from dreamor.utils.logging import Logger, class_name_to_file_name, mkdir, cp_files\n",
    "from dreamor.utils.torch import get_device, save_state, load_state\n",
    "from dreamor.utils.stats import StatTracker\n",
    "from dreamor.models.model_utils import step\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "160908ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default: {'beta1', 'beta2', 'ckpt', 'use_adam', 'eps', 'load_optim', 'decay'}\n",
      "Using default: {'output_delta', 'ddim_steps', 'model_use_smpl_joint_inputs', 'detach_sched_samp'}\n",
      "Using default: {'splits_path', 'frames_out_step_size', 'data_noise_std'}\n",
      "Using default: set()\n"
     ]
    }
   ],
   "source": [
    "config_file = \"configs/train_diffusion_transformer.yaml\"\n",
    "config_parser_yaml = ConfigParser(config_file)\n",
    "args_obj, _ = config_parser_yaml.parse('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f53ace0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_dict: {'dataset': 'AmassDiscreteDataset', 'model': 'DreamorDiffusionTransformer', 'loss': 'DiffusionLoss', 'out': './out/diffusiontransformer_train', 'ckpt': None, 'gpu': 0, 'batch_size': 128, 'print_every': 10, 'epochs': 200, 'val_every': 2, 'save_every': 25, 'lr': 0.001, 'beta1': 0.9, 'beta2': 0.999, 'eps': 1e-08, 'sched_milestones': [70, 120, 170], 'sched_decay': 0.5, 'decay': 0.0, 'load_optim': True, 'use_adam': False, 'sched_samp_start': -1, 'sched_samp_end': -2}\n",
      "model_dict: {'out_rot_rep': 'aa', 'in_rot_rep': 'mat', 'latent_size': 128, 'steps_in': 1, 'output_delta': True, 'model_data_config': 'smpl+joints+contacts', 'detach_sched_samp': True, 'model_use_smpl_joint_inputs': False, 'pose_token_dim': 64, 'diffusion_base_dim': 256, 'nhead': 4, 'num_layers': 6, 'dim_feedforward': 1024, 'dropout': 0.1, 'cfg_scale': 4.0, 'cond_drop_prob': 0.1, 'use_mean_sample': True, 'ddim_steps': 100}\n",
      "dataset_dict: {'data_paths': ['../datasets/AMASS/amass_processed'], 'split_by': 'sequence', 'splits_path': None, 'sample_num_frames': 10, 'data_rot_rep': 'mat', 'step_frames_in': 1, 'step_frames_out': 1, 'frames_out_step_size': 1, 'data_return_config': 'smpl+joints+contacts', 'data_noise_std': 0.0}\n",
      "loss_dict: {'ddpm': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# See config\n",
    "dict_attr = ['base_dict', 'model_dict', 'dataset_dict', 'loss_dict']\n",
    "for attr in dict_attr:\n",
    "    print(f\"{attr}: {getattr(args_obj, attr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cba45594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base args: namespace(dataset='AmassDiscreteDataset', model='DreamorDiffusionTransformer', loss='DiffusionLoss', out='./out/diffusiontransformer_train\\\\20250512_105715', ckpt=None, gpu=0, batch_size=128, print_every=10, epochs=200, val_every=2, save_every=25, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-08, sched_milestones=[70, 120, 170], sched_decay=0.5, decay=0.0, load_optim=True, use_adam=False, sched_samp_start=-1, sched_samp_end=-2)\n",
      "Model args: namespace(out_rot_rep='aa', in_rot_rep='mat', latent_size=128, steps_in=1, output_delta=True, model_data_config='smpl+joints+contacts', detach_sched_samp=True, model_use_smpl_joint_inputs=False, pose_token_dim=64, diffusion_base_dim=256, nhead=4, num_layers=6, dim_feedforward=1024, dropout=0.1, cfg_scale=4.0, cond_drop_prob=0.1, use_mean_sample=True, ddim_steps=100)\n",
      "Dataset args: namespace(data_paths=['../datasets/AMASS/amass_processed'], split_by='sequence', splits_path=None, sample_num_frames=10, data_rot_rep='mat', step_frames_in=1, step_frames_out=1, frames_out_step_size=1, data_return_config='smpl+joints+contacts', data_noise_std=0.0)\n",
      "Loss args: namespace(ddpm=1.0)\n"
     ]
    }
   ],
   "source": [
    "args = args_obj.base\n",
    "args.out = os.path.join(args.out, time.strftime('%Y%m%d_%H%M%S'))\n",
    "mkdir(args.out)\n",
    "train_log_path = os.path.join(args.out, 'train.log')\n",
    "Logger.init(train_log_path)\n",
    "   \n",
    "# save arguments used\n",
    "Logger.log('Base args: ' + str(args))\n",
    "Logger.log('Model args: ' + str(args_obj.model))\n",
    "Logger.log('Dataset args: ' + str(args_obj.dataset))\n",
    "Logger.log('Loss args: ' + str(args_obj.loss))\n",
    "\n",
    "train_scripts_path = os.path.join(args.out, 'train_scripts')\n",
    "mkdir(train_scripts_path)\n",
    "pkg_root = \"dreamor\"\n",
    "dataset_file = class_name_to_file_name(args.dataset)\n",
    "dataset_file_path = os.path.join(pkg_root, 'datasets/' + dataset_file + '.py')\n",
    "model_file = class_name_to_file_name(args.model)\n",
    "model_file_path = os.path.join(pkg_root, 'models/' + model_file + '.py')\n",
    "loss_file = class_name_to_file_name(args.loss)\n",
    "train_file_path = \"train.ipynb\"\n",
    "cp_files(train_scripts_path, [train_file_path, model_file_path, dataset_file_path, config_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e77626ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if training Diffusion\n",
    "vae_cfg = 'configs/train_motion_vae.yaml'\n",
    "vae_ckpt = 'out/motion_vae/20250506_014121/checkpoints/best_train_model.pth'\n",
    "\n",
    "# # colab\n",
    "# vae_cfg = '/content/drive/MyDrive/DiffuMoR/pretrained_models/motion_vae_128/train_motion_vae.yaml'\n",
    "# vae_ckpt = '/content/drive/MyDrive/DiffuMoR/pretrained_models/motion_vae_128/best_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13e55124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model class:  <module 'models.dreamor_diffusion_transformer' from 'E:\\\\workspace\\\\Motion\\\\humor\\\\dreamor\\\\utils\\\\..\\\\models\\\\dreamor_diffusion_transformer.py'>\n",
      "Using default: {'beta1', 'beta2', 'ckpt', 'use_adam', 'eps', 'load_optim', 'decay'}\n",
      "Using default: {'output_delta', 'model_use_smpl_joint_inputs', 'detach_sched_samp'}\n",
      "Using default: {'splits_path', 'frames_out_step_size', 'data_noise_std'}\n",
      "Using default: {'kl_loss_cycle_len', 'smpl_vert_consistency_loss'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\workspace\\Motion\\humor\\dreamor\\utils\\torch.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  full_checkpoint_dict = torch.load(load_path, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "model_class = importlib.import_module('models.' + model_file)\n",
    "print('Model class: ', model_class)\n",
    "Model = getattr(model_class, args.model)\n",
    "\n",
    "if args.model == 'DreamorDiffusionTransformer':\n",
    "    model = Model(**args_obj.model_dict,\n",
    "                    vae_cfg=vae_cfg, vae_ckpt=vae_ckpt,\n",
    "                    model_smpl_batch_size=args.batch_size)\n",
    "else:\n",
    "    model = Model(**args_obj.model_dict,\n",
    "                  model_smpl_batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1313e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss class:  <module 'losses.diffusion_loss' from 'E:\\\\workspace\\\\Motion\\\\humor\\\\dreamor\\\\utils\\\\..\\\\losses\\\\diffusion_loss.py'>\n"
     ]
    }
   ],
   "source": [
    "  # load loss class and instantiate\n",
    "loss_class = importlib.import_module('losses.' + loss_file)\n",
    "print('Loss class: ', loss_class)\n",
    "Loss = getattr(loss_class, args.loss)\n",
    "if args.loss == 'DiffusionLoss':\n",
    "    loss_func = Loss(**args_obj.loss_dict) # assumes loss is DiffusionLoss\n",
    "else:\n",
    "    loss_func = Loss(**args_obj.loss_dict,\n",
    "                      smpl_batch_size=args.batch_size*args_obj.dataset.sample_num_frames) # assumes loss is HumorLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "641c2f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using detected GPU...\n",
      "DreamorDiffusionTransformer(\n",
      "  (diffusion_model): DiffusionTransformer(\n",
      "    (pose_tokenizer): PoseTokenizer(\n",
      "      (part_proj): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=3, out_features=64, bias=True)\n",
      "        (2): Linear(in_features=9, out_features=64, bias=True)\n",
      "        (3): Linear(in_features=3, out_features=64, bias=True)\n",
      "        (4): Linear(in_features=189, out_features=64, bias=True)\n",
      "        (5-6): 2 x Linear(in_features=66, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (latent_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (pose_proj): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (time_embed): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    )\n",
      "    (transformer): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (motion_vae): MotionVAE(\n",
      "    (encoder): MLP(\n",
      "      (net): ModuleList(\n",
      "        (0): Linear(in_features=678, out_features=1024, bias=True)\n",
      "        (1): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (4): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
      "        (5): ReLU()\n",
      "        (6): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (7): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
      "        (8): ReLU()\n",
      "        (9): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (10): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
      "        (11): ReLU()\n",
      "        (12): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (decoder): MLP(\n",
      "      (net): ModuleList(\n",
      "        (0): Linear(in_features=467, out_features=1024, bias=True)\n",
      "        (1): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=1152, out_features=1024, bias=True)\n",
      "        (4): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
      "        (5): ReLU()\n",
      "        (6): Linear(in_features=1152, out_features=512, bias=True)\n",
      "        (7): GroupNorm(16, 512, eps=1e-05, affine=True)\n",
      "        (8): ReLU()\n",
      "        (9): Linear(in_features=640, out_features=216, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num model params: 5372672\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = get_device(args.gpu)\n",
    "model.to(device)\n",
    "loss_func.to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# count params in model\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "Logger.log('Num model params: ' + str(params))\n",
    "\n",
    "# freeze params in loss\n",
    "for param in loss_func.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93b1fca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not use scheduled sampling with given start and end!\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "betas = (args.beta1, args.beta2)\n",
    "if args.use_adam:\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                            lr=args.lr,\n",
    "                            betas=betas,\n",
    "                            eps=args.eps,\n",
    "                            weight_decay=args.decay)\n",
    "else:\n",
    "    optimizer = optim.Adamax(model.parameters(),\n",
    "                            lr=args.lr,\n",
    "                            betas=betas,\n",
    "                            eps=args.eps,\n",
    "                            weight_decay=args.decay)\n",
    "\n",
    "# load in pretrained weights/optimizer state if given\n",
    "start_epoch = 0\n",
    "min_val_loss = min_train_loss = float('inf')\n",
    "if args.ckpt is not None:\n",
    "    load_optim = optimizer if args.load_optim else None\n",
    "    start_epoch, min_val_loss, min_train_loss = load_state(args.ckpt, model, optimizer=load_optim, map_location=device, ignore_keys=model.ignore_keys)\n",
    "    start_epoch += 1\n",
    "    Logger.log('Resuming from saved checkpoint at epoch idx %d with min val loss %.6f...' % (start_epoch, min_val_loss))\n",
    "    if not args.load_optim:\n",
    "        Logger.log('Not loading optimizer state as desired...')\n",
    "        Logger.log('WARNING: Also resetting min_val_loss and epoch count!')\n",
    "        min_val_loss = float('inf')\n",
    "        start_epoch = 0\n",
    "\n",
    "# initialize LR scheduler\n",
    "scheduler = MultiStepLR(optimizer, milestones=args.sched_milestones, gamma=args.sched_decay)\n",
    "\n",
    "# intialize schedule sampling if desired\n",
    "use_sched_samp = False\n",
    "if args.sched_samp_start is not None and args.sched_samp_end is not None:\n",
    "    if args.sched_samp_start >= 0 and args.sched_samp_end >= args.sched_samp_start:\n",
    "        Logger.log('Using scheduled sampling starting at epoch %d and ending at epoch %d!' % (args.sched_samp_start, args.sched_samp_end))\n",
    "        use_sched_samp = True\n",
    "    else:\n",
    "        Logger.log('Could not use scheduled sampling with given start and end!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "035ae111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class:  <class 'datasets.amass_discrete_dataset.AmassDiscreteDataset'>\n",
      "Loading data from../datasets/AMASS/amass_processed\n",
      "Logger must be initialized before logging!\n",
      "This split contains 2425 sequences (that meet the duration criteria).\n",
      "Logger must be initialized before logging!\n",
      "The dataset contains 106399 sub-sequences in total.\n",
      "Logger must be initialized before logging!\n",
      "Loading data from../datasets/AMASS/amass_processed\n",
      "Logger must be initialized before logging!\n",
      "This split contains 246 sequences (that meet the duration criteria).\n",
      "Logger must be initialized before logging!\n",
      "The dataset contains 10247 sub-sequences in total.\n",
      "Logger must be initialized before logging!\n"
     ]
    }
   ],
   "source": [
    "# load dataset class and instantiate training and validation set\n",
    "Dataset = getattr(importlib.import_module('datasets.' + dataset_file), args.dataset)\n",
    "print('Dataset class: ', Dataset)\n",
    "train_dataset = Dataset(split='train', **args_obj.dataset_dict)\n",
    "val_dataset = Dataset(split='val', **args_obj.dataset_dict)\n",
    "# create loaders\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=args.batch_size,\n",
    "                        shuffle=False, \n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93bd7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats tracker\n",
    "tensorboard_path = os.path.join(args.out, 'train_tensorboard')\n",
    "mkdir(tensorboard_path)\n",
    "stat_tracker = StatTracker(tensorboard_path, train_log_path)\n",
    "\n",
    "# checkpoints saving\n",
    "ckpts_path = os.path.join(args.out, 'checkpoints')\n",
    "mkdir(ckpts_path)\n",
    "\n",
    "if use_sched_samp:\n",
    "    train_dataset.return_global = True\n",
    "    val_dataset.return_global = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce692d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop\n",
    "train_start_t = time.time()\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # train\n",
    "    stat_tracker.reset()\n",
    "    batch_start_t = None\n",
    "    reset_loss_track = train_dataset.pre_batch(epoch=epoch)\n",
    "    # see which phase we're in \n",
    "    sched_samp_gt_p = 1.0 # supervised\n",
    "    if use_sched_samp:\n",
    "        if epoch >= args.sched_samp_start and epoch < args.sched_samp_end:\n",
    "            frac = (epoch - args.sched_samp_start) / (args.sched_samp_end - args.sched_samp_start)\n",
    "            sched_samp_gt_p = 1.0*(1.0 - frac)\n",
    "        elif epoch >= args.sched_samp_end:\n",
    "            # autoregressive\n",
    "            sched_samp_gt_p = 0.0\n",
    "        Logger.log('Scheduled sampling current use_gt_p = %f' % (sched_samp_gt_p))\n",
    "\n",
    "        if epoch == args.sched_samp_end:\n",
    "            # the loss will naturally go up when using own rollouts\n",
    "            reset_loss_track = True\n",
    "\n",
    "        if args_obj.loss_dict['kl_loss_cycle_len'] > 0:\n",
    "            # if we're cycling, only want to save results when using full ELBO\n",
    "            if (epoch % args_obj.loss_dict['kl_loss_cycle_len']) > (args_obj.loss_dict['kl_loss_cycle_len'] // 2):\n",
    "                # have reached second half of a cycle\n",
    "                reset_loss_track = True\n",
    "\n",
    "    if reset_loss_track:\n",
    "        Logger.log('Resetting min_val_loss and min_train_loss')\n",
    "        min_val_loss = min_train_loss = float('inf')\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        batch_start_t = time.time()\n",
    "\n",
    "        try:\n",
    "            # zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # run model\n",
    "            loss, stats_dict = step(model, loss_func, data, train_dataset, device, epoch, mode='train', use_gt_p=sched_samp_gt_p)\n",
    "            if torch.isnan(loss).item():\n",
    "                Logger.log('WARNING: NaN loss. Skipping to next data...')\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            # backprop and step\n",
    "            loss.backward()\n",
    "            # check gradients\n",
    "            parameters = [p for p in model.parameters() if p.grad is not None]\n",
    "            total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), 2.0).to(device) for p in parameters]), 2.0)\n",
    "            if torch.isnan(total_norm):\n",
    "                Logger.log('WARNING: NaN gradients. Skipping to next data...')\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            optimizer.step()\n",
    "        except (RuntimeError, AssertionError) as e:\n",
    "            if epoch > 0:\n",
    "                # to catch bad dynamics, but keep training\n",
    "                Logger.log('WARNING: caught an exception during forward or backward pass. Skipping to next data...')\n",
    "                Logger.log(e)\n",
    "                traceback.print_exc()\n",
    "                reset_loss_track = train_dataset.pre_batch(epoch=epoch)\n",
    "                if reset_loss_track:\n",
    "                    Logger.log('Resetting min_val_loss and min_train_loss')\n",
    "                    min_val_loss = min_train_loss = float('inf')\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "        # collect stats\n",
    "        batch_elapsed_t = time.time() - batch_start_t\n",
    "        total_elapsed_t = time.time() - train_start_t\n",
    "        stats_dict['loss'] = loss\n",
    "        for param_group in optimizer.param_groups:\n",
    "            stats_dict['lr'] = torch.Tensor([param_group['lr']])[0]\n",
    "        stats_dict['time_per_batch'] = torch.Tensor([batch_elapsed_t])[0]\n",
    "\n",
    "        last_batch = (i==(len(train_loader)-1))\n",
    "        stat_tracker.update(stats_dict, tag='train', save_tf=last_batch)\n",
    "        if i % args.print_every == 0:\n",
    "            stat_tracker.print(i, len(train_loader),\n",
    "                            epoch, args.epochs,\n",
    "                            total_elapsed_time=total_elapsed_t,\n",
    "                            tag='train')\n",
    "\n",
    "        reset_loss_track = train_dataset.pre_batch(epoch=epoch)\n",
    "        if reset_loss_track:\n",
    "            Logger.log('Resetting min_val_loss and min_train_loss')\n",
    "            min_val_loss = min_train_loss = float('inf')\n",
    "\n",
    "    # save if desired\n",
    "    if epoch % args.save_every == 0:\n",
    "        Logger.log('Saving checkpoint...')\n",
    "        save_file = os.path.join(ckpts_path, 'epoch_%08d_model.pth' % (epoch))\n",
    "        save_state(save_file, model, optimizer, cur_epoch=epoch, min_val_loss=min_val_loss, min_train_loss=min_train_loss, ignore_keys=model.ignore_keys)\n",
    "\n",
    "    # check if it's the best train model so far\n",
    "    mean_train_loss = stat_tracker.meter_dict['train/loss'].avg\n",
    "    if mean_train_loss < min_train_loss:\n",
    "        min_train_loss = mean_train_loss\n",
    "        Logger.log('Best train loss so far! Saving checkpoint...')\n",
    "        save_file = os.path.join(ckpts_path, 'best_train_model.pth')\n",
    "        save_state(save_file, model, optimizer, cur_epoch=epoch, min_val_loss=min_val_loss, min_train_loss=min_train_loss, ignore_keys=model.ignore_keys)\n",
    "\n",
    "    # validate\n",
    "    if epoch % args.val_every == 0:\n",
    "        with torch.no_grad():\n",
    "            # run on validation data\n",
    "            model.eval()\n",
    "\n",
    "            stat_tracker.reset()\n",
    "            for i, data in enumerate(val_loader):\n",
    "                # print(i)\n",
    "                batch_start_t = time.time()\n",
    "                # run model\n",
    "                loss, stats_dict = step(model, loss_func, data, val_dataset, device, epoch, mode='test', use_gt_p=sched_samp_gt_p)\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    Logger.log('WARNING: NaN loss on VALIDATION. Skipping to next data...')\n",
    "                    continue\n",
    "\n",
    "                # collect stats\n",
    "                batch_elapsed_t = time.time() - batch_start_t\n",
    "                total_elapsed_t = time.time() - train_start_t\n",
    "                stats_dict['loss'] = loss\n",
    "                stats_dict['time_per_batch'] = torch.Tensor([batch_elapsed_t])[0]\n",
    "\n",
    "                stat_tracker.update(stats_dict, tag='val', save_tf=(i==(len(val_loader)-1)), increment_step=False)\n",
    "\n",
    "                if i % args.print_every == 0:\n",
    "                    stat_tracker.print(i, len(val_loader),\n",
    "                                    epoch, args.epochs,\n",
    "                                    total_elapsed_time=total_elapsed_t,\n",
    "                                    tag='val')\n",
    "\n",
    "            # check if it's the best model so far\n",
    "            mean_val_loss = stat_tracker.meter_dict['val/loss'].avg\n",
    "            if mean_val_loss < min_val_loss:\n",
    "                min_val_loss = mean_val_loss\n",
    "                Logger.log('Best val loss so far! Saving checkpoint...')\n",
    "                save_file = os.path.join(ckpts_path, 'best_model.pth')\n",
    "                save_state(save_file, model, optimizer, cur_epoch=epoch, min_val_loss=min_val_loss, min_train_loss=min_train_loss, ignore_keys=model.ignore_keys)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "Logger.log('Finished!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS280",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
